{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "blockquote { background: #AEDE94; }\n",
       "h1 { \n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "h2 { \n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "\tbackground-color: #ffcccc;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "div.exercise-r {\n",
       "\tbackground-color: #fce8e8;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "\n",
       "span.sub-q {\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "div.theme {\n",
       "\tbackground-color: #DDDDDD;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 18pt;\n",
       "}\n",
       "div.gc { \n",
       "\tbackground-color: #AEDE94;\n",
       "\tborder-color: #E9967A; \t \n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 12pt;\n",
       "}\n",
       "p.q1 { \n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RUN THIS CELL \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\n",
    "    \"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\"\n",
    ").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import functools\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(style='whitegrid')\n",
    "pd.set_option('display.width', 1500)\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a progressbar function\n",
    "def progressbar(n_step, n_total):\n",
    "    \"\"\"Prints self-updating progress bar to stdout to track for-loop progress\n",
    "    \n",
    "    There are entire 3rd-party libraries dedicated to custom progress-bars.\n",
    "    A simple function like this is often more than enough to get the job done.\n",
    "    \n",
    "    :param n_total: total number of expected for-loop iterations\n",
    "    :type n_total: int\n",
    "    :param n_step: current iteration number, starting at 0\n",
    "    :type n_step: int\n",
    "\n",
    "    .. example::\n",
    "    \n",
    "        for i in range(n_iterations):\n",
    "            progressbar(i, n_iterations)\n",
    "            \n",
    "    .. source:\n",
    "    \n",
    "        This function is a simplified version of code found here:\n",
    "        https://stackoverflow.com/questions/3160699/python-progress-bar/15860757#15860757\n",
    "    \"\"\"\n",
    "    n_step = n_step + 1\n",
    "    barlen = 50\n",
    "    progress = n_step / n_total\n",
    "    block = int(round(barlen * progress))\n",
    "    status = \"\"\n",
    "    if n_step == n_total:\n",
    "        status = \"Done...\\r\\n\\n\"\n",
    "    text = \"\\r [{0}] {1}/{2} {3}\".format(\n",
    "        \"=\" * block + \"-\" * (barlen - block),\n",
    "        n_step,\n",
    "        n_total,\n",
    "        status,\n",
    "    )\n",
    "    sys.stdout.write(text)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create the dataset for Part C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryuic\\Anaconda3\\envs\\cs109a\\lib\\site-packages\\pandas\\core\\series.py:726: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('data/df_fifa.csv')\n",
    "\n",
    "# select players in the following clubs as test set\n",
    "df_c = df[df['d_div1_league']==1]\n",
    "\n",
    "# extract league and club\n",
    "df_league_club = df_c.copy()\n",
    "df_league_club = df_league_club[['league_name','club']].drop_duplicates()\n",
    "df_league_club['variable'] = 'club_' + df_league_club['club']\n",
    "\n",
    "# create lead variables for target variables (values in the following year)\n",
    "target_vars = ['overall','value_eur','skill_moves','d_multiple_position','d_trait_Injury_Prone']\n",
    "added_vars = []\n",
    "\n",
    "df_c = df_c.sort_values(['sofifa_id','year'])\n",
    "for i, target_var in enumerate(target_vars):\n",
    "    # insert lead variables next to original ones\n",
    "    col_index = list(df_c.columns).index(target_var) + 1\n",
    "    next_var = target_var + '_next'\n",
    "    diff_var = target_var + '_diff'\n",
    "    df_c.insert(col_index, next_var, df_c.groupby(['sofifa_id'])[target_var].shift(-1))\n",
    "    \n",
    "    # calculate change to the following years\n",
    "    # for value, calculate percentage change as distribution is highly skewed\n",
    "    if target_var == 'value_eur':\n",
    "        df_c.insert(col_index + 1, diff_var, np.log(df_c[next_var]) - np.log(df_c[target_var]))\n",
    "    else:\n",
    "        df_c.insert(col_index + 1, diff_var, df_c[next_var] - df_c[target_var])\n",
    "        \n",
    "    added_vars.append(next_var)\n",
    "    added_vars.append(diff_var)\n",
    "\n",
    "# create dummies for club, nationality, \n",
    "df_c_dummies = pd.get_dummies(df_c[['nationality','club','work_rate','league_name']])\n",
    "df_c = pd.concat([df_c, df_c_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drop columns, imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c.to_csv('data/df_c.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnecessary columns\n",
    "drop_vars = ['sofifa_id','short_name','dob','nationality','club','wage_eur','preferred_foot','work_rate','body_type',\n",
    "             'team_position','team_jersey_number','joined','contract_valid_until','league_name','release_clause_eur',\n",
    "             'loaned_from','main_position','year']\n",
    "df_c_all = df_c.drop(drop_vars, axis=1)\n",
    "\n",
    "# impute zero values for fieldplayers and goalkeeping ability\n",
    "# this is because goalkeeping ability is not available for field players and vice versa\n",
    "impute_vars = ['ab_pace','ab_shooting','ab_passing','ab_dribbling','ab_defending','ab_physic'] + [x for x in df_c_all.columns if x.startswith('ab_gk')]\n",
    "for var in impute_vars:\n",
    "    df_c_all[var] = df_c_all[var].fillna(0)\n",
    "    \n",
    "# impute mean value for ab_mentality_conposure\n",
    "# Composure is a Player Attribute in FIFA that determines a player's the state or feeling of being calm and \n",
    "# controlling their frustration in matches frustration. (from FIFAplay)\n",
    "df_c_all['ab_mentality_composure'].fillna(df_c_all['ab_mentality_composure'].mean(), inplace=True)\n",
    "\n",
    "# save to csv\n",
    "df_c_all.to_csv('data/df_c_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic idea  \n",
    "- We define a club's ability to increase a player stats and value as the club's performance when we control the player's basic characteristics, e.g. age, reputation, skills, etc. \n",
    "- Given this, we regress players' overall score (annual change) on all these characteristics and club dummies, and identify the feature importance of a club dummy as the club's performance when controlling other factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter data, standardization, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10517, 361)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate a dataset for overall score regression and drop NA\n",
    "section_target = 'overall'\n",
    "added_vars_temp = [x for x in added_vars if not x == section_target + '_diff']\n",
    "df_c_ovr = df_c_all.copy().drop(added_vars_temp, axis=1)\n",
    "df_c_ovr = df_c_ovr.drop('value_eur', axis=1)\n",
    "df_c_ovr = df_c_ovr.dropna()\n",
    "\n",
    "# save to csv\n",
    "df_c_ovr.to_csv('data/df_c_ovr.csv', index=False)\n",
    "df_c_ovr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign X and y\n",
    "X = df_c_ovr.drop([section_target, section_target + '_diff'], axis=1)\n",
    "y = df_c_ovr[section_target + '_diff']\n",
    "\n",
    "# standardization\n",
    "scaler = StandardScaler().fit(X)\n",
    "X_stan = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [==================================================] 4/4 Done...\n",
      "\n",
      "6.743067001152627 0.01\n"
     ]
    }
   ],
   "source": [
    "# grid search\n",
    "# set parameters\n",
    "la_alphas = [1e-2, 1e-1, 1, 1e+1]\n",
    "\n",
    "# create empty lists to store errors\n",
    "la_tr_err, la_val_err = [],[]\n",
    "\n",
    "# run regression for each alpha\n",
    "for i,alpha in enumerate(la_alphas):\n",
    "    # update progressbar\n",
    "    progressbar(i, len(la_alphas))\n",
    "    \n",
    "    # perform cross-validation on the training data with 10 folds and get the mse_scores\n",
    "    lasso = Lasso(alpha=alpha, max_iter=10000)\n",
    "    scores = cross_validate(lasso, X_stan, y, \n",
    "                            cv=5, \n",
    "                            scoring='neg_mean_squared_error', \n",
    "                            return_train_score=True,\n",
    "                            n_jobs=-1)\n",
    "    \n",
    "    #Compute the train and validation MSE\n",
    "    la_tr_err.append(scores['train_score'].mean() * -1)\n",
    "    la_val_err.append(scores['test_score'].mean() * -1)\n",
    "\n",
    "# find the degree that returns the minimum validation error\n",
    "la_min_val_err = min(la_val_err)\n",
    "la_best_alpha = la_alphas[la_val_err.index(la_min_val_err)]\n",
    "print(la_min_val_err, la_best_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=0.01, max_iter=10000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# refit Lasso using best alpha\n",
    "la_best = Lasso(alpha=la_best_alpha, max_iter=10000)\n",
    "la_best.fit(X_stan, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store coefficients into dataframe\n",
    "df_ovr_la_fi = pd.DataFrame({'variable':X.columns,\n",
    "                             'ovr_la':la_best.coef_})\n",
    "df_ovr_la_fi = df_ovr_la_fi[df_ovr_la_fi['variable'].str.startswith('club')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest (m=p/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 77 candidates, totalling 385 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed: 10.4min\n",
      "[Parallel(n_jobs=-1)]: Done 385 out of 385 | elapsed: 27.4min finished\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "rf_trees = list(range(100, 450, 50))\n",
    "rf_depths = list(range(5, 16, 1))\n",
    " \n",
    "rf_params = {'n_estimators': rf_trees, \n",
    "             'max_depth': rf_depths}\n",
    " \n",
    "# grid search\n",
    "rf = RandomForestRegressor(warm_start=True,max_features=int(X_stan.shape[1]/3),random_state=0)\n",
    "rf_gs = GridSearchCV(estimator=rf,param_grid=rf_params,scoring='neg_mean_squared_error',verbose=1,n_jobs=-1)\n",
    "rf_gs.fit(X_stan, y)\n",
    "\n",
    "# extract best parameters and estimator\n",
    "rf_best_param = rf_gs.best_params_\n",
    "rf_best_estimator = rf_gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select club dummies\n",
    "clubs = [x for x in X.columns if x.startswith('club')]\n",
    "\n",
    "# compute feature importance\n",
    "# for each club, compute the difference of predicted values when the club dummy = 1 and 0\n",
    "# compute change in response variable for all players\n",
    "X_fi = pd.DataFrame(data=X_stan,\n",
    "                    columns=X.columns)\n",
    "\n",
    "# all club dummies = 0 (Since already standardized, impute the minimum value for each club)\n",
    "X_clubs_min = X_fi[clubs].min()\n",
    "X_clubs_max = X_fi[clubs].max()\n",
    "X_fi_zero = X_fi.copy()\n",
    "for club in clubs:\n",
    "    X_fi_zero[clubs] = X_clubs_min[club]\n",
    "\n",
    "rf_fi = []\n",
    "for club in clubs:\n",
    "    \n",
    "    # dummy for the seleted club = 1 (similarly, impute the max value for the club)\n",
    "    X_fi_one = X_fi_zero.copy()\n",
    "    X_fi_one[club] = X_clubs_max[club]\n",
    "    \n",
    "    # predict for each dataframe, take mean and take difference\n",
    "    pred_one = rf_best_estimator.predict(X_fi_one).mean()\n",
    "    pred_zero = rf_best_estimator.predict(X_fi_zero).mean()\n",
    "    pred_diff = pred_one - pred_zero\n",
    "    \n",
    "    # append\n",
    "    rf_fi.append(pred_diff)\n",
    "\n",
    "# store coefficients into dataframe\n",
    "df_ovr_rf_fi = pd.DataFrame({'variable':clubs,\n",
    "                             'ovr_rf':rf_fi})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "param_space = {'min_child_weight': hp.loguniform('min_child_weight', np.log(1), np.log(10)),\n",
    "               'max_depth': hp.quniform('max_depth', 3, 9, 1),\n",
    "               'subsample': hp.quniform('subsample', 0.6, 0.95, 0.05),\n",
    "               'colsample_bytree': hp.quniform('colsample_bytree', 0.6, 0.95, 0.05),\n",
    "               'gamma': hp.loguniform('gamma', np.log(1e-8), np.log(1.0)),\n",
    "               'reg_alpha': hp.loguniform('reg_alpha', np.log(1e-8), np.log(1.0)),\n",
    "               'reg_lambda': hp.loguniform('reg_lambda', np.log(1e-6), np.log(10.0))}\n",
    "\n",
    "# define score function\n",
    "def score(params):\n",
    "    xgb = XGBRegressor(random_state=81,\n",
    "                       min_child_weight=params['min_child_weight'],\n",
    "                       max_depth=int(params['max_depth']), \n",
    "                       subsample=params['subsample'], \n",
    "                       colsample_bytree=params['colsample_bytree'],\n",
    "                       gamma=params['gamma'], \n",
    "                       reg_alpha=params['reg_alpha'], \n",
    "                       reg_lambda=params['reg_lambda'])\n",
    "    scores = cross_validate(xgb, X_stan, y, \n",
    "                            cv=5, \n",
    "                            scoring='neg_mean_squared_error', \n",
    "                            n_jobs=-1)\n",
    "    return -1 * scores['test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████| 100/100 [14:47<00:00,  8.88s/trial, best loss: 0.3183672646921562]\n"
     ]
    }
   ],
   "source": [
    "# run gridsearch and find best parameters\n",
    "max_evals = 100\n",
    "trials = Trials()\n",
    "history = []\n",
    "rstate = np.random.RandomState(81)\n",
    "best_params = fmin(score, param_space, algo=tpe.suggest, trials=trials, max_evals=max_evals, rstate=rstate)\n",
    "\n",
    "# refit with the best parameters\n",
    "xgb_best = XGBRegressor(random_state=81,\n",
    "                        min_child_weight=best_params['min_child_weight'],\n",
    "                        max_depth=int(best_params['max_depth']), \n",
    "                        subsample=best_params['subsample'], \n",
    "                        colsample_bytree=best_params['colsample_bytree'],\n",
    "                        gamma=best_params['gamma'], \n",
    "                        reg_alpha=best_params['reg_alpha'], \n",
    "                        reg_lambda=best_params['reg_lambda'])\n",
    "xgb_best.fit(X_stan, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute feature importance\n",
    "xgb_fi = []\n",
    "for club in clubs:\n",
    "    \n",
    "    # dummy for the seleted club = 1 (similarly, impute the max value for the club)\n",
    "    X_fi_one = X_fi_zero.copy()\n",
    "    X_fi_one[club] = X_clubs_max[club]\n",
    "    \n",
    "    # predict for each dataframe, take mean and take difference\n",
    "    pred_one = xgb_best.predict(X_fi_one.to_numpy()).mean()\n",
    "    pred_zero = xgb_best.predict(X_fi_zero.to_numpy()).mean()\n",
    "    pred_diff = pred_one - pred_zero\n",
    "    \n",
    "    # append\n",
    "    xgb_fi.append(pred_diff)\n",
    "\n",
    "# store coefficients into dataframe\n",
    "df_ovr_xgb_fi = pd.DataFrame({'variable':clubs,\n",
    "                              'ovr_xgb':xgb_fi})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Market value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter data, standardization, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8432, 362)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate a dataset for market value regression and drop NA\n",
    "section_target = 'value_eur'\n",
    "added_vars_temp = [x for x in added_vars if not x == section_target + '_diff']\n",
    "df_c_value = df_c_all.copy().drop(added_vars_temp, axis=1)\n",
    "df_c_value = df_c_value.replace([np.inf, -np.inf], np.nan)\n",
    "df_c_value = df_c_value.dropna()\n",
    "\n",
    "# save to csv\n",
    "df_c_value.to_csv('data/df_c_value.csv', index=False)\n",
    "df_c_value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign X and y\n",
    "X = df_c_value.drop([section_target, section_target + '_diff'], axis=1)\n",
    "y = df_c_value[section_target + '_diff']\n",
    "\n",
    "# standardization\n",
    "scaler = StandardScaler().fit(X)\n",
    "X_stan = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [==================================================] 4/4 Done...\n",
      "\n",
      "0.3110163303647024 0.01\n"
     ]
    }
   ],
   "source": [
    "# grid search\n",
    "# set parameters\n",
    "la_alphas = [1e-2, 1e-1, 1, 1e+1]\n",
    "\n",
    "# create empty lists to store errors\n",
    "la_tr_err, la_val_err = [],[]\n",
    "\n",
    "# run regression for each alpha\n",
    "for i,alpha in enumerate(la_alphas):\n",
    "    # update progressbar\n",
    "    progressbar(i, len(la_alphas))\n",
    "    \n",
    "    # perform cross-validation on the training data with 10 folds and get the mse_scores\n",
    "    lasso = Lasso(alpha=alpha, max_iter=10000)\n",
    "    scores = cross_validate(lasso, X_stan, y, \n",
    "                            cv=5, \n",
    "                            scoring='neg_mean_squared_error', \n",
    "                            return_train_score=True,\n",
    "                            n_jobs=-1)\n",
    "    \n",
    "    #Compute the train and validation MSE\n",
    "    la_tr_err.append(scores['train_score'].mean() * -1)\n",
    "    la_val_err.append(scores['test_score'].mean() * -1)\n",
    "\n",
    "# find the degree that returns the minimum validation error\n",
    "la_min_val_err = min(la_val_err)\n",
    "la_best_alpha = la_alphas[la_val_err.index(la_min_val_err)]\n",
    "print(la_min_val_err, la_best_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=0.01, max_iter=10000)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# refit Lasso using best alpha\n",
    "la_best = Lasso(alpha=la_best_alpha, max_iter=10000)\n",
    "la_best.fit(X_stan, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store coefficients into dataframe\n",
    "df_value_la_fi = pd.DataFrame({'variable':X.columns,\n",
    "                               'value_la':la_best.coef_})\n",
    "df_value_la_fi = df_value_la_fi[df_value_la_fi['variable'].str.startswith('club')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest (m=p/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 77 candidates, totalling 385 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  9.1min\n",
      "[Parallel(n_jobs=-1)]: Done 385 out of 385 | elapsed: 25.1min finished\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "rf_trees = list(range(100, 450, 50))\n",
    "rf_depths = list(range(5, 16, 1))\n",
    " \n",
    "rf_params = {'n_estimators': rf_trees, \n",
    "             'max_depth': rf_depths}\n",
    " \n",
    "# grid search\n",
    "rf = RandomForestRegressor(warm_start=True,max_features=int(X_stan.shape[1]/3),random_state=0)\n",
    "rf_gs = GridSearchCV(estimator=rf,param_grid=rf_params,scoring='neg_mean_squared_error',verbose=1,n_jobs=-1)\n",
    "rf_gs.fit(X_stan, y)\n",
    "\n",
    "# extract best parameters and estimator\n",
    "rf_best_param = rf_gs.best_params_\n",
    "rf_best_estimator = rf_gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select club dummies\n",
    "clubs = [x for x in X.columns if x.startswith('club')]\n",
    "\n",
    "# compute feature importance\n",
    "# for each club, compute the difference of predicted values when the club dummy = 1 and 0\n",
    "# compute change in response variable for all players\n",
    "X_fi = pd.DataFrame(data=X_stan,\n",
    "                    columns=X.columns)\n",
    "\n",
    "# all club dummies = 0 (Since already standardized, impute the minimum value for each club)\n",
    "X_clubs_min = X_fi[clubs].min()\n",
    "X_clubs_max = X_fi[clubs].max()\n",
    "X_fi_zero = X_fi.copy()\n",
    "for club in clubs:\n",
    "    X_fi_zero[clubs] = X_clubs_min[club]\n",
    "\n",
    "rf_fi = []\n",
    "for club in clubs:\n",
    "    \n",
    "    # dummy for the seleted club = 1 (similarly, impute the max value for the club)\n",
    "    X_fi_one = X_fi_zero.copy()\n",
    "    X_fi_one[club] = X_clubs_max[club]\n",
    "    \n",
    "    # predict for each dataframe, take mean and take difference\n",
    "    pred_one = rf_best_estimator.predict(X_fi_one).mean()\n",
    "    pred_zero = rf_best_estimator.predict(X_fi_zero).mean()\n",
    "    pred_diff = pred_one - pred_zero\n",
    "    \n",
    "    # append\n",
    "    rf_fi.append(pred_diff)\n",
    "\n",
    "# store coefficients into dataframe\n",
    "df_value_rf_fi = pd.DataFrame({'variable':clubs,\n",
    "                               'value_rf':rf_fi})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "param_space = {'min_child_weight': hp.loguniform('min_child_weight', np.log(1), np.log(10)),\n",
    "               'max_depth': hp.quniform('max_depth', 3, 9, 1),\n",
    "               'subsample': hp.quniform('subsample', 0.6, 0.95, 0.05),\n",
    "               'colsample_bytree': hp.quniform('colsample_bytree', 0.6, 0.95, 0.05),\n",
    "               'gamma': hp.loguniform('gamma', np.log(1e-8), np.log(1.0)),\n",
    "               'reg_alpha': hp.loguniform('reg_alpha', np.log(1e-8), np.log(1.0)),\n",
    "               'reg_lambda': hp.loguniform('reg_lambda', np.log(1e-6), np.log(10.0))}\n",
    "\n",
    "# define score function\n",
    "def score(params):\n",
    "    xgb = XGBRegressor(random_state=81,\n",
    "                       min_child_weight=params['min_child_weight'],\n",
    "                       max_depth=int(params['max_depth']), \n",
    "                       subsample=params['subsample'], \n",
    "                       colsample_bytree=params['colsample_bytree'],\n",
    "                       gamma=params['gamma'], \n",
    "                       reg_alpha=params['reg_alpha'], \n",
    "                       reg_lambda=params['reg_lambda'])\n",
    "    scores = cross_validate(xgb, X_stan, y, \n",
    "                            cv=5, \n",
    "                            scoring='neg_mean_squared_error', \n",
    "                            n_jobs=-1)\n",
    "    return -1 * scores['test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████| 100/100 [14:47<00:00,  8.88s/trial, best loss: 0.3183672646921562]\n"
     ]
    }
   ],
   "source": [
    "# run gridsearch and find best parameters\n",
    "max_evals = 100\n",
    "trials = Trials()\n",
    "history = []\n",
    "rstate = np.random.RandomState(81)\n",
    "best_params = fmin(score, param_space, algo=tpe.suggest, trials=trials, max_evals=max_evals, rstate=rstate)\n",
    "\n",
    "# refit with the best parameters\n",
    "xgb_best = XGBRegressor(random_state=81,\n",
    "                        min_child_weight=best_params['min_child_weight'],\n",
    "                        max_depth=int(best_params['max_depth']), \n",
    "                        subsample=best_params['subsample'], \n",
    "                        colsample_bytree=best_params['colsample_bytree'],\n",
    "                        gamma=best_params['gamma'], \n",
    "                        reg_alpha=best_params['reg_alpha'], \n",
    "                        reg_lambda=best_params['reg_lambda'])\n",
    "xgb_best.fit(X_stan, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute feature importance\n",
    "xgb_fi = []\n",
    "for club in clubs:\n",
    "    \n",
    "    # dummy for the seleted club = 1 (similarly, impute the max value for the club)\n",
    "    X_fi_one = X_fi_zero.copy()\n",
    "    X_fi_one[club] = X_clubs_max[club]\n",
    "    \n",
    "    # predict for each dataframe, take mean and take difference\n",
    "    pred_one = xgb_best.predict(X_fi_one.to_numpy()).mean()\n",
    "    pred_zero = xgb_best.predict(X_fi_zero.to_numpy()).mean()\n",
    "    pred_diff = pred_one - pred_zero\n",
    "    \n",
    "    # append\n",
    "    xgb_fi.append(pred_diff)\n",
    "\n",
    "# store coefficients into dataframe\n",
    "df_value_xgb_fi = pd.DataFrame({'variable':clubs,\n",
    "                                'value_xgb':xgb_fi})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skill move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter data, standardization, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10517, 361)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate a dataset for overall score regression and drop NA\n",
    "section_target = 'skill_moves'\n",
    "added_vars_temp = [x for x in added_vars if not x == section_target + '_diff']\n",
    "df_c_skill = df_c_all.copy().drop(added_vars_temp, axis=1)\n",
    "df_c_skill = df_c_skill.drop('value_eur', axis=1)\n",
    "df_c_skill = df_c_skill.dropna()\n",
    "\n",
    "# save to csv\n",
    "df_c_skill.to_csv('data/df_c_skill.csv', index=False)\n",
    "df_c_skill.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign X and y\n",
    "X = df_c_skill.drop([section_target, section_target + '_diff'], axis=1)\n",
    "y = df_c_skill[section_target + '_diff']\n",
    "\n",
    "# standardization\n",
    "scaler = StandardScaler().fit(X)\n",
    "X_stan = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [==================================================] 4/4 Done...\n",
      "\n",
      "6.743067001152627 0.01\n"
     ]
    }
   ],
   "source": [
    "# grid search\n",
    "# set parameters\n",
    "la_alphas = [1e-2, 1e-1, 1, 1e+1]\n",
    "\n",
    "# create empty lists to store errors\n",
    "la_tr_err, la_val_err = [],[]\n",
    "\n",
    "# run regression for each alpha\n",
    "for i,alpha in enumerate(la_alphas):\n",
    "    # update progressbar\n",
    "    progressbar(i, len(la_alphas))\n",
    "    \n",
    "    # perform cross-validation on the training data with 10 folds and get the mse_scores\n",
    "    lasso = Lasso(alpha=alpha, max_iter=10000)\n",
    "    scores = cross_validate(lasso, X_stan, y, \n",
    "                            cv=5, \n",
    "                            scoring='neg_mean_squared_error', \n",
    "                            return_train_score=True,\n",
    "                            n_jobs=-1)\n",
    "    \n",
    "    #Compute the train and validation MSE\n",
    "    la_tr_err.append(scores['train_score'].mean() * -1)\n",
    "    la_val_err.append(scores['test_score'].mean() * -1)\n",
    "\n",
    "# find the degree that returns the minimum validation error\n",
    "la_min_val_err = min(la_val_err)\n",
    "la_best_alpha = la_alphas[la_val_err.index(la_min_val_err)]\n",
    "print(la_min_val_err, la_best_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=0.01, max_iter=10000)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# refit Lasso using best alpha\n",
    "la_best = Lasso(alpha=la_best_alpha, max_iter=10000)\n",
    "la_best.fit(X_stan, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store coefficients into dataframe\n",
    "df_skill_la_fi = pd.DataFrame({'variable':X.columns,\n",
    "                               'skill_la':la_best.coef_})\n",
    "df_skill_la_fi = df_skill_la_fi[df_skill_la_fi['variable'].str.startswith('club')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest (m=p/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 77 candidates, totalling 385 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed: 10.4min\n",
      "[Parallel(n_jobs=-1)]: Done 385 out of 385 | elapsed: 27.4min finished\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "rf_trees = list(range(100, 450, 50))\n",
    "rf_depths = list(range(5, 16, 1))\n",
    " \n",
    "rf_params = {'n_estimators': rf_trees, \n",
    "             'max_depth': rf_depths}\n",
    " \n",
    "# grid search\n",
    "rf = RandomForestRegressor(warm_start=True,max_features=int(X_stan.shape[1]/3),random_state=0)\n",
    "rf_gs = GridSearchCV(estimator=rf,param_grid=rf_params,scoring='neg_mean_squared_error',verbose=1,n_jobs=-1)\n",
    "rf_gs.fit(X_stan, y)\n",
    "\n",
    "# extract best parameters and estimator\n",
    "rf_best_param = rf_gs.best_params_\n",
    "rf_best_estimator = rf_gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select club dummies\n",
    "clubs = [x for x in X.columns if x.startswith('club')]\n",
    "\n",
    "# compute feature importance\n",
    "# for each club, compute the difference of predicted values when the club dummy = 1 and 0\n",
    "# compute change in response variable for all players\n",
    "X_fi = pd.DataFrame(data=X_stan,\n",
    "                    columns=X.columns)\n",
    "\n",
    "# all club dummies = 0 (Since already standardized, impute the minimum value for each club)\n",
    "X_clubs_min = X_fi[clubs].min()\n",
    "X_clubs_max = X_fi[clubs].max()\n",
    "X_fi_zero = X_fi.copy()\n",
    "for club in clubs:\n",
    "    X_fi_zero[clubs] = X_clubs_min[club]\n",
    "\n",
    "rf_fi = []\n",
    "for club in clubs:\n",
    "    \n",
    "    # dummy for the seleted club = 1 (similarly, impute the max value for the club)\n",
    "    X_fi_one = X_fi_zero.copy()\n",
    "    X_fi_one[club] = X_clubs_max[club]\n",
    "    \n",
    "    # predict for each dataframe, take mean and take difference\n",
    "    pred_one = rf_best_estimator.predict(X_fi_one).mean()\n",
    "    pred_zero = rf_best_estimator.predict(X_fi_zero).mean()\n",
    "    pred_diff = pred_one - pred_zero\n",
    "    \n",
    "    # append\n",
    "    rf_fi.append(pred_diff)\n",
    "\n",
    "# store coefficients into dataframe\n",
    "df_skill_rf_fi = pd.DataFrame({'variable':clubs,\n",
    "                               'skill_rf':rf_fi})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "param_space = {'min_child_weight': hp.loguniform('min_child_weight', np.log(1), np.log(10)),\n",
    "               'max_depth': hp.quniform('max_depth', 3, 9, 1),\n",
    "               'subsample': hp.quniform('subsample', 0.6, 0.95, 0.05),\n",
    "               'colsample_bytree': hp.quniform('colsample_bytree', 0.6, 0.95, 0.05),\n",
    "               'gamma': hp.loguniform('gamma', np.log(1e-8), np.log(1.0)),\n",
    "               'reg_alpha': hp.loguniform('reg_alpha', np.log(1e-8), np.log(1.0)),\n",
    "               'reg_lambda': hp.loguniform('reg_lambda', np.log(1e-6), np.log(10.0))}\n",
    "\n",
    "# define score function\n",
    "def score(params):\n",
    "    xgb = XGBRegressor(random_state=81,\n",
    "                       min_child_weight=params['min_child_weight'],\n",
    "                       max_depth=int(params['max_depth']), \n",
    "                       subsample=params['subsample'], \n",
    "                       colsample_bytree=params['colsample_bytree'],\n",
    "                       gamma=params['gamma'], \n",
    "                       reg_alpha=params['reg_alpha'], \n",
    "                       reg_lambda=params['reg_lambda'])\n",
    "    scores = cross_validate(xgb, X_stan, y, \n",
    "                            cv=5, \n",
    "                            scoring='neg_mean_squared_error', \n",
    "                            n_jobs=-1)\n",
    "    return -1 * scores['test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████| 100/100 [14:47<00:00,  8.88s/trial, best loss: 0.3183672646921562]\n"
     ]
    }
   ],
   "source": [
    "# run gridsearch and find best parameters\n",
    "max_evals = 100\n",
    "trials = Trials()\n",
    "history = []\n",
    "rstate = np.random.RandomState(81)\n",
    "best_params = fmin(score, param_space, algo=tpe.suggest, trials=trials, max_evals=max_evals, rstate=rstate)\n",
    "\n",
    "# refit with the best parameters\n",
    "xgb_best = XGBRegressor(random_state=81,\n",
    "                        min_child_weight=best_params['min_child_weight'],\n",
    "                        max_depth=int(best_params['max_depth']), \n",
    "                        subsample=best_params['subsample'], \n",
    "                        colsample_bytree=best_params['colsample_bytree'],\n",
    "                        gamma=best_params['gamma'], \n",
    "                        reg_alpha=best_params['reg_alpha'], \n",
    "                        reg_lambda=best_params['reg_lambda'])\n",
    "xgb_best.fit(X_stan, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute feature importance\n",
    "xgb_fi = []\n",
    "for club in clubs:\n",
    "    \n",
    "    # dummy for the seleted club = 1 (similarly, impute the max value for the club)\n",
    "    X_fi_one = X_fi_zero.copy()\n",
    "    X_fi_one[club] = X_clubs_max[club]\n",
    "    \n",
    "    # predict for each dataframe, take mean and take difference\n",
    "    pred_one = xgb_best.predict(X_fi_one.to_numpy()).mean()\n",
    "    pred_zero = xgb_best.predict(X_fi_zero.to_numpy()).mean()\n",
    "    pred_diff = pred_one - pred_zero\n",
    "    \n",
    "    # append\n",
    "    xgb_fi.append(pred_diff)\n",
    "\n",
    "# store coefficients into dataframe\n",
    "df_skill_xgb_fi = pd.DataFrame({'variable':clubs,\n",
    "                                'skill_xgb':xgb_fi})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter data, standardization, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10517, 361)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate a dataset for overall score regression and drop NA\n",
    "section_target = 'd_multiple_position'\n",
    "added_vars_temp = [x for x in added_vars if not x == section_target + '_diff']\n",
    "df_c_util = df_c_all.copy().drop(added_vars_temp, axis=1)\n",
    "df_c_util = df_c_util.drop('value_eur', axis=1)\n",
    "df_c_util = df_c_util.dropna()\n",
    "\n",
    "# save to csv\n",
    "df_c_util.to_csv('data/df_c_util.csv', index=False)\n",
    "df_c_util.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign X and y\n",
    "X = df_c_util.drop([section_target, section_target + '_diff'], axis=1)\n",
    "y = df_c_util[section_target + '_diff']\n",
    "\n",
    "# standardization\n",
    "scaler = StandardScaler().fit(X)\n",
    "X_stan = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [==================================================] 4/4 Done...\n",
      "\n",
      "0.13260380364012925 0.01\n"
     ]
    }
   ],
   "source": [
    "# grid search\n",
    "# set parameters\n",
    "la_alphas = [1e-2, 1e-1, 1, 1e+1]\n",
    "\n",
    "# create empty lists to store errors\n",
    "la_tr_err, la_val_err = [],[]\n",
    "\n",
    "# run regression for each alpha\n",
    "for i,alpha in enumerate(la_alphas):\n",
    "    # update progressbar\n",
    "    progressbar(i, len(la_alphas))\n",
    "    \n",
    "    # perform cross-validation on the training data with 10 folds and get the mse_scores\n",
    "    lasso = Lasso(alpha=alpha, max_iter=10000)\n",
    "    scores = cross_validate(lasso, X_stan, y, \n",
    "                            cv=5, \n",
    "                            scoring='neg_mean_squared_error', \n",
    "                            return_train_score=True,\n",
    "                            n_jobs=-1)\n",
    "    \n",
    "    #Compute the train and validation MSE\n",
    "    la_tr_err.append(scores['train_score'].mean() * -1)\n",
    "    la_val_err.append(scores['test_score'].mean() * -1)\n",
    "\n",
    "# find the degree that returns the minimum validation error\n",
    "la_min_val_err = min(la_val_err)\n",
    "la_best_alpha = la_alphas[la_val_err.index(la_min_val_err)]\n",
    "print(la_min_val_err, la_best_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=0.01, max_iter=10000)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# refit Lasso using best alpha\n",
    "la_best = Lasso(alpha=la_best_alpha, max_iter=10000)\n",
    "la_best.fit(X_stan, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store coefficients into dataframe\n",
    "df_util_la_fi = pd.DataFrame({'variable':X.columns,\n",
    "                              'util_la':la_best.coef_})\n",
    "df_util_la_fi = df_util_la_fi[df_util_la_fi['variable'].str.startswith('club')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest (m=p/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 77 candidates, totalling 385 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed: 10.4min\n",
      "[Parallel(n_jobs=-1)]: Done 385 out of 385 | elapsed: 27.4min finished\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "rf_trees = list(range(100, 450, 50))\n",
    "rf_depths = list(range(5, 16, 1))\n",
    " \n",
    "rf_params = {'n_estimators': rf_trees, \n",
    "             'max_depth': rf_depths}\n",
    " \n",
    "# grid search\n",
    "rf = RandomForestRegressor(warm_start=True,max_features=int(X_stan.shape[1]/3),random_state=0)\n",
    "rf_gs = GridSearchCV(estimator=rf,param_grid=rf_params,scoring='neg_mean_squared_error',verbose=1,n_jobs=-1)\n",
    "rf_gs.fit(X_stan, y)\n",
    "\n",
    "# extract best parameters and estimator\n",
    "rf_best_param = rf_gs.best_params_\n",
    "rf_best_estimator = rf_gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select club dummies\n",
    "clubs = [x for x in X.columns if x.startswith('club')]\n",
    "\n",
    "# compute feature importance\n",
    "# for each club, compute the difference of predicted values when the club dummy = 1 and 0\n",
    "# compute change in response variable for all players\n",
    "X_fi = pd.DataFrame(data=X_stan,\n",
    "                    columns=X.columns)\n",
    "\n",
    "# all club dummies = 0 (Since already standardized, impute the minimum value for each club)\n",
    "X_clubs_min = X_fi[clubs].min()\n",
    "X_clubs_max = X_fi[clubs].max()\n",
    "X_fi_zero = X_fi.copy()\n",
    "for club in clubs:\n",
    "    X_fi_zero[clubs] = X_clubs_min[club]\n",
    "\n",
    "rf_fi = []\n",
    "for club in clubs:\n",
    "    \n",
    "    # dummy for the seleted club = 1 (similarly, impute the max value for the club)\n",
    "    X_fi_one = X_fi_zero.copy()\n",
    "    X_fi_one[club] = X_clubs_max[club]\n",
    "    \n",
    "    # predict for each dataframe, take mean and take difference\n",
    "    pred_one = rf_best_estimator.predict(X_fi_one).mean()\n",
    "    pred_zero = rf_best_estimator.predict(X_fi_zero).mean()\n",
    "    pred_diff = pred_one - pred_zero\n",
    "    \n",
    "    # append\n",
    "    rf_fi.append(pred_diff)\n",
    "\n",
    "# store coefficients into dataframe\n",
    "df_util_rf_fi = pd.DataFrame({'variable':clubs,\n",
    "                              'util_rf':rf_fi})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "param_space = {'min_child_weight': hp.loguniform('min_child_weight', np.log(1), np.log(10)),\n",
    "               'max_depth': hp.quniform('max_depth', 3, 9, 1),\n",
    "               'subsample': hp.quniform('subsample', 0.6, 0.95, 0.05),\n",
    "               'colsample_bytree': hp.quniform('colsample_bytree', 0.6, 0.95, 0.05),\n",
    "               'gamma': hp.loguniform('gamma', np.log(1e-8), np.log(1.0)),\n",
    "               'reg_alpha': hp.loguniform('reg_alpha', np.log(1e-8), np.log(1.0)),\n",
    "               'reg_lambda': hp.loguniform('reg_lambda', np.log(1e-6), np.log(10.0))}\n",
    "\n",
    "# define score function\n",
    "def score(params):\n",
    "    xgb = XGBRegressor(random_state=81,\n",
    "                       min_child_weight=params['min_child_weight'],\n",
    "                       max_depth=int(params['max_depth']), \n",
    "                       subsample=params['subsample'], \n",
    "                       colsample_bytree=params['colsample_bytree'],\n",
    "                       gamma=params['gamma'], \n",
    "                       reg_alpha=params['reg_alpha'], \n",
    "                       reg_lambda=params['reg_lambda'])\n",
    "    scores = cross_validate(xgb, \n",
    "                            X=X_stan, \n",
    "                            y=y, \n",
    "                            cv=5, \n",
    "                            scoring='neg_mean_squared_error', \n",
    "                            n_jobs=-1)\n",
    "    return -1 * scores['test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████| 100/100 [14:47<00:00,  8.88s/trial, best loss: 0.3183672646921562]\n"
     ]
    }
   ],
   "source": [
    "# run gridsearch and find best parameters\n",
    "max_evals = 100\n",
    "trials = Trials()\n",
    "history = []\n",
    "rstate = np.random.RandomState(81)\n",
    "best_params = fmin(score, param_space, algo=tpe.suggest, trials=trials, max_evals=max_evals, rstate=rstate)\n",
    "\n",
    "# refit with the best parameters\n",
    "xgb_best = XGBRegressor(random_state=81,\n",
    "                        min_child_weight=best_params['min_child_weight'],\n",
    "                        max_depth=int(best_params['max_depth']), \n",
    "                        subsample=best_params['subsample'], \n",
    "                        colsample_bytree=best_params['colsample_bytree'],\n",
    "                        gamma=best_params['gamma'], \n",
    "                        reg_alpha=best_params['reg_alpha'], \n",
    "                        reg_lambda=best_params['reg_lambda'])\n",
    "xgb_best.fit(X_stan, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute feature importance\n",
    "xgb_fi = []\n",
    "for club in clubs:\n",
    "    \n",
    "    # dummy for the seleted club = 1 (similarly, impute the max value for the club)\n",
    "    X_fi_one = X_fi_zero.copy()\n",
    "    X_fi_one[club] = X_clubs_max[club]\n",
    "    \n",
    "    # predict for each dataframe, take mean and take difference\n",
    "    pred_one = xgb_best.predict(X_fi_one.to_numpy()).mean()\n",
    "    pred_zero = xgb_best.predict(X_fi_zero.to_numpy()).mean()\n",
    "    pred_diff = pred_one - pred_zero\n",
    "    \n",
    "    # append\n",
    "    xgb_fi.append(pred_diff)\n",
    "\n",
    "# store coefficients into dataframe\n",
    "df_util_xgb_fi = pd.DataFrame({'variable':clubs,\n",
    "                              'util_xgb':xgb_fi})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Injury Proneness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter data, standardization, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10517, 361)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate a dataset for overall score regression and drop NA\n",
    "section_target = 'd_trait_Injury_Prone'\n",
    "added_vars_temp = [x for x in added_vars if not x == section_target + '_diff']\n",
    "df_c_injury = df_c_all.copy().drop(added_vars_temp, axis=1)\n",
    "df_c_injury = df_c_injury.drop('value_eur', axis=1)\n",
    "df_c_injury = df_c_injury.dropna()\n",
    "\n",
    "# save to csv\n",
    "df_c_injury.to_csv('data/df_c_injury.csv', index=False)\n",
    "df_c_injury.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign X and y\n",
    "X = df_c_injury.drop([section_target, section_target + '_diff'], axis=1)\n",
    "y = df_c_injury[section_target + '_diff']\n",
    "\n",
    "# standardization\n",
    "scaler = StandardScaler().fit(X)\n",
    "X_stan = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [==================================================] 4/4 Done...\n",
      "\n",
      "0.06269139096831858 0.01\n"
     ]
    }
   ],
   "source": [
    "# grid search\n",
    "# set parameters\n",
    "la_alphas = [1e-2, 1e-1, 1, 1e+1]\n",
    "\n",
    "# create empty lists to store errors\n",
    "la_tr_err, la_val_err = [],[]\n",
    "\n",
    "# run regression for each alpha\n",
    "for i,alpha in enumerate(la_alphas):\n",
    "    # update progressbar\n",
    "    progressbar(i, len(la_alphas))\n",
    "    \n",
    "    # perform cross-validation on the training data with 10 folds and get the mse_scores\n",
    "    lasso = Lasso(alpha=alpha, max_iter=10000)\n",
    "    scores = cross_validate(lasso, X_stan, y, \n",
    "                            cv=5, \n",
    "                            scoring='neg_mean_squared_error', \n",
    "                            return_train_score=True,\n",
    "                            n_jobs=-1)\n",
    "    \n",
    "    #Compute the train and validation MSE\n",
    "    la_tr_err.append(scores['train_score'].mean() * -1)\n",
    "    la_val_err.append(scores['test_score'].mean() * -1)\n",
    "\n",
    "# find the degree that returns the minimum validation error\n",
    "la_min_val_err = min(la_val_err)\n",
    "la_best_alpha = la_alphas[la_val_err.index(la_min_val_err)]\n",
    "print(la_min_val_err, la_best_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=0.01, max_iter=10000)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# refit Lasso using best alpha\n",
    "la_best = Lasso(alpha=la_best_alpha, max_iter=10000)\n",
    "la_best.fit(X_stan, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store coefficients into dataframe\n",
    "df_injury_la_fi = pd.DataFrame({'variable':X.columns,\n",
    "                                'injury_la':-1*(la_best.coef_)})\n",
    "df_injury_la_fi = df_injury_la_fi[df_injury_la_fi['variable'].str.startswith('club')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest (m=p/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of   5 | elapsed:    8.2s remaining:   12.3s\n",
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:    8.3s finished\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "rf_trees = list(range(100, 450, 50))\n",
    "rf_depths = list(range(5, 16, 1))\n",
    " \n",
    "rf_params = {'n_estimators': rf_trees, \n",
    "             'max_depth': rf_depths}\n",
    " \n",
    "# grid search\n",
    "rf = RandomForestRegressor(warm_start=True,max_features=int(X_stan.shape[1]/3),random_state=0)\n",
    "rf_gs = GridSearchCV(estimator=rf,param_grid=rf_params,scoring='neg_mean_squared_error',verbose=1,n_jobs=-1)\n",
    "rf_gs.fit(X_stan, y)\n",
    "\n",
    "# extract best parameters and estimator\n",
    "rf_best_param = rf_gs.best_params_\n",
    "rf_best_estimator = rf_gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select club dummies\n",
    "clubs = [x for x in X.columns if x.startswith('club')]\n",
    "\n",
    "# compute feature importance\n",
    "# for each club, compute the difference of predicted values when the club dummy = 1 and 0\n",
    "# compute change in response variable for all players\n",
    "X_fi = pd.DataFrame(data=X_stan,\n",
    "                    columns=X.columns)\n",
    "\n",
    "# all club dummies = 0 (Since already standardized, impute the minimum value for each club)\n",
    "X_clubs_min = X_fi[clubs].min()\n",
    "X_clubs_max = X_fi[clubs].max()\n",
    "X_fi_zero = X_fi.copy()\n",
    "for club in clubs:\n",
    "    X_fi_zero[clubs] = X_clubs_min[club]\n",
    "\n",
    "rf_fi = []\n",
    "for club in clubs:\n",
    "    \n",
    "    # dummy for the seleted club = 1 (similarly, impute the max value for the club)\n",
    "    X_fi_one = X_fi_zero.copy()\n",
    "    X_fi_one[club] = X_clubs_max[club]\n",
    "    \n",
    "    # predict for each dataframe, take mean and take difference\n",
    "    pred_one = rf_best_estimator.predict(X_fi_one).mean()\n",
    "    pred_zero = rf_best_estimator.predict(X_fi_zero).mean()\n",
    "    pred_diff = -1*(pred_one - pred_zero)\n",
    "    \n",
    "    # append\n",
    "    rf_fi.append(pred_diff)\n",
    "\n",
    "# store coefficients into dataframe\n",
    "df_injury_rf_fi = pd.DataFrame({'variable':clubs,\n",
    "                                'injury_rf':rf_fi})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "param_space = {'min_child_weight': hp.loguniform('min_child_weight', np.log(1), np.log(10)),\n",
    "               'max_depth': hp.quniform('max_depth', 3, 9, 1),\n",
    "               'subsample': hp.quniform('subsample', 0.6, 0.95, 0.05),\n",
    "               'colsample_bytree': hp.quniform('colsample_bytree', 0.6, 0.95, 0.05),\n",
    "               'gamma': hp.loguniform('gamma', np.log(1e-8), np.log(1.0)),\n",
    "               'reg_alpha': hp.loguniform('reg_alpha', np.log(1e-8), np.log(1.0)),\n",
    "               'reg_lambda': hp.loguniform('reg_lambda', np.log(1e-6), np.log(10.0))}\n",
    "\n",
    "# define score function\n",
    "def score(params):\n",
    "    xgb = XGBRegressor(random_state=81,\n",
    "                       min_child_weight=params['min_child_weight'],\n",
    "                       max_depth=int(params['max_depth']), \n",
    "                       subsample=params['subsample'], \n",
    "                       colsample_bytree=params['colsample_bytree'],\n",
    "                       gamma=params['gamma'], \n",
    "                       reg_alpha=params['reg_alpha'], \n",
    "                       reg_lambda=params['reg_lambda'])\n",
    "    scores = cross_validate(xgb, \n",
    "                            X=X_stan, \n",
    "                            y=y, \n",
    "                            cv=5, \n",
    "                            scoring='neg_mean_squared_error', \n",
    "                            n_jobs=-1)\n",
    "    return -1 * scores['test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████| 100/100 [14:47<00:00,  8.88s/trial, best loss: 0.3183672646921562]\n"
     ]
    }
   ],
   "source": [
    "# run gridsearch and find best parameters\n",
    "max_evals = 100\n",
    "trials = Trials()\n",
    "history = []\n",
    "rstate = np.random.RandomState(81)\n",
    "best_params = fmin(score, param_space, algo=tpe.suggest, trials=trials, max_evals=max_evals, rstate=rstate)\n",
    "\n",
    "# refit with the best parameters\n",
    "xgb_best = XGBRegressor(random_state=81,\n",
    "                        min_child_weight=best_params['min_child_weight'],\n",
    "                        max_depth=int(best_params['max_depth']), \n",
    "                        subsample=best_params['subsample'], \n",
    "                        colsample_bytree=best_params['colsample_bytree'],\n",
    "                        gamma=best_params['gamma'], \n",
    "                        reg_alpha=best_params['reg_alpha'], \n",
    "                        reg_lambda=best_params['reg_lambda'])\n",
    "xgb_best.fit(X_stan, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute feature importance\n",
    "xgb_fi = []\n",
    "for club in clubs:\n",
    "    \n",
    "    # dummy for the seleted club = 1 (similarly, impute the max value for the club)\n",
    "    X_fi_one = X_fi_zero.copy()\n",
    "    X_fi_one[club] = X_clubs_max[club]\n",
    "    \n",
    "    # predict for each dataframe, take mean and take difference\n",
    "    pred_one = xgb_best.predict(X_fi_one.to_numpy()).mean()\n",
    "    pred_zero = xgb_best.predict(X_fi_zero.to_numpy()).mean()\n",
    "    pred_diff = pred_one - pred_zero\n",
    "    \n",
    "    # append\n",
    "    xgb_fi.append(pred_diff)\n",
    "\n",
    "# store coefficients into dataframe\n",
    "df_injury_xgb_fi = pd.DataFrame({'variable':clubs,\n",
    "                                 'injury_xgb':xgb_fi})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge all feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>ovr_la</th>\n",
       "      <th>ovr_rf</th>\n",
       "      <th>ovr_xgb</th>\n",
       "      <th>value_la</th>\n",
       "      <th>value_rf</th>\n",
       "      <th>value_xgb</th>\n",
       "      <th>ovr_la_rescale</th>\n",
       "      <th>ovr_rf_rescale</th>\n",
       "      <th>ovr_xgb_rescale</th>\n",
       "      <th>value_la_rescale</th>\n",
       "      <th>value_rf_rescale</th>\n",
       "      <th>value_xgb_rescale</th>\n",
       "      <th>ovr_mean</th>\n",
       "      <th>value_mean</th>\n",
       "      <th>all_mean</th>\n",
       "      <th>league_name</th>\n",
       "      <th>club</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>club_RCD Espanyol</td>\n",
       "      <td>0.063244</td>\n",
       "      <td>0.157218</td>\n",
       "      <td>0.359391</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006468</td>\n",
       "      <td>0.066094</td>\n",
       "      <td>79.706246</td>\n",
       "      <td>54.983483</td>\n",
       "      <td>77.510157</td>\n",
       "      <td>47.745949</td>\n",
       "      <td>56.646203</td>\n",
       "      <td>69.282974</td>\n",
       "      <td>70.733295</td>\n",
       "      <td>57.891709</td>\n",
       "      <td>64.312502</td>\n",
       "      <td>Spain Primera Division</td>\n",
       "      <td>RCD Espanyol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>club_Real Madrid</td>\n",
       "      <td>0.055333</td>\n",
       "      <td>0.048552</td>\n",
       "      <td>0.242421</td>\n",
       "      <td>0.008895</td>\n",
       "      <td>0.018871</td>\n",
       "      <td>0.045970</td>\n",
       "      <td>76.499878</td>\n",
       "      <td>31.284066</td>\n",
       "      <td>70.772737</td>\n",
       "      <td>68.860774</td>\n",
       "      <td>65.529877</td>\n",
       "      <td>65.198635</td>\n",
       "      <td>59.518894</td>\n",
       "      <td>66.529762</td>\n",
       "      <td>63.024328</td>\n",
       "      <td>Spain Primera Division</td>\n",
       "      <td>Real Madrid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>club_Real Betis</td>\n",
       "      <td>0.024710</td>\n",
       "      <td>0.055922</td>\n",
       "      <td>0.424391</td>\n",
       "      <td>0.002140</td>\n",
       "      <td>0.012304</td>\n",
       "      <td>0.083840</td>\n",
       "      <td>64.087678</td>\n",
       "      <td>32.891463</td>\n",
       "      <td>81.254149</td>\n",
       "      <td>52.826595</td>\n",
       "      <td>60.826080</td>\n",
       "      <td>72.884567</td>\n",
       "      <td>59.411097</td>\n",
       "      <td>62.179081</td>\n",
       "      <td>60.795089</td>\n",
       "      <td>Spain Primera Division</td>\n",
       "      <td>Real Betis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>club_RCD Mallorca</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.091834</td>\n",
       "      <td>0.102545</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012612</td>\n",
       "      <td>0.106051</td>\n",
       "      <td>54.468118</td>\n",
       "      <td>40.723529</td>\n",
       "      <td>62.715911</td>\n",
       "      <td>47.745949</td>\n",
       "      <td>61.047155</td>\n",
       "      <td>77.392412</td>\n",
       "      <td>52.635853</td>\n",
       "      <td>62.061839</td>\n",
       "      <td>57.348846</td>\n",
       "      <td>Spain Primera Division</td>\n",
       "      <td>RCD Mallorca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>club_Sevilla FC</td>\n",
       "      <td>0.025476</td>\n",
       "      <td>0.055922</td>\n",
       "      <td>0.221568</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023001</td>\n",
       "      <td>0.013451</td>\n",
       "      <td>64.397854</td>\n",
       "      <td>32.891434</td>\n",
       "      <td>69.571598</td>\n",
       "      <td>47.745949</td>\n",
       "      <td>68.488231</td>\n",
       "      <td>58.598604</td>\n",
       "      <td>55.620295</td>\n",
       "      <td>58.277595</td>\n",
       "      <td>56.948945</td>\n",
       "      <td>Spain Primera Division</td>\n",
       "      <td>Sevilla FC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>club_West Ham United</td>\n",
       "      <td>-0.065329</td>\n",
       "      <td>-0.000411</td>\n",
       "      <td>-0.025274</td>\n",
       "      <td>-0.001318</td>\n",
       "      <td>-0.000820</td>\n",
       "      <td>-0.054099</td>\n",
       "      <td>27.592683</td>\n",
       "      <td>20.605453</td>\n",
       "      <td>55.353563</td>\n",
       "      <td>44.616755</td>\n",
       "      <td>51.426052</td>\n",
       "      <td>44.888880</td>\n",
       "      <td>34.517233</td>\n",
       "      <td>46.977229</td>\n",
       "      <td>40.747231</td>\n",
       "      <td>English Premier League</td>\n",
       "      <td>West Ham United</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>club_Brighton &amp; Hove Albion</td>\n",
       "      <td>-0.073050</td>\n",
       "      <td>-0.007629</td>\n",
       "      <td>-0.368933</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.001372</td>\n",
       "      <td>-0.052932</td>\n",
       "      <td>24.463182</td>\n",
       "      <td>19.031131</td>\n",
       "      <td>35.558836</td>\n",
       "      <td>47.745949</td>\n",
       "      <td>51.030969</td>\n",
       "      <td>45.125647</td>\n",
       "      <td>26.351050</td>\n",
       "      <td>47.967521</td>\n",
       "      <td>37.159286</td>\n",
       "      <td>English Premier League</td>\n",
       "      <td>Brighton &amp; Hove Albion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>club_Newcastle United</td>\n",
       "      <td>-0.073136</td>\n",
       "      <td>-0.009587</td>\n",
       "      <td>-0.123237</td>\n",
       "      <td>-0.007101</td>\n",
       "      <td>-0.003831</td>\n",
       "      <td>-0.066593</td>\n",
       "      <td>24.428359</td>\n",
       "      <td>18.604284</td>\n",
       "      <td>49.710892</td>\n",
       "      <td>30.890788</td>\n",
       "      <td>49.269546</td>\n",
       "      <td>42.353088</td>\n",
       "      <td>30.914512</td>\n",
       "      <td>40.837807</td>\n",
       "      <td>35.876159</td>\n",
       "      <td>English Premier League</td>\n",
       "      <td>Newcastle United</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>club_Crystal Palace</td>\n",
       "      <td>-0.079612</td>\n",
       "      <td>-0.003805</td>\n",
       "      <td>-0.639170</td>\n",
       "      <td>-0.007251</td>\n",
       "      <td>-0.001766</td>\n",
       "      <td>-0.027649</td>\n",
       "      <td>21.803584</td>\n",
       "      <td>19.865117</td>\n",
       "      <td>19.993252</td>\n",
       "      <td>30.533734</td>\n",
       "      <td>50.748752</td>\n",
       "      <td>50.257037</td>\n",
       "      <td>20.553984</td>\n",
       "      <td>43.846508</td>\n",
       "      <td>32.200246</td>\n",
       "      <td>English Premier League</td>\n",
       "      <td>Crystal Palace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>club_Aston Villa</td>\n",
       "      <td>-0.097596</td>\n",
       "      <td>-0.017283</td>\n",
       "      <td>-0.209804</td>\n",
       "      <td>-0.016957</td>\n",
       "      <td>-0.017323</td>\n",
       "      <td>-0.125454</td>\n",
       "      <td>14.514385</td>\n",
       "      <td>16.925695</td>\n",
       "      <td>44.724653</td>\n",
       "      <td>7.495359</td>\n",
       "      <td>39.605456</td>\n",
       "      <td>30.406669</td>\n",
       "      <td>25.388244</td>\n",
       "      <td>25.835828</td>\n",
       "      <td>25.612036</td>\n",
       "      <td>English Premier League</td>\n",
       "      <td>Aston Villa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       variable    ovr_la    ovr_rf   ovr_xgb  value_la  value_rf  value_xgb  ovr_la_rescale  ovr_rf_rescale  ovr_xgb_rescale  value_la_rescale  value_rf_rescale  value_xgb_rescale   ovr_mean  value_mean   all_mean             league_name                    club\n",
       "67            club_RCD Espanyol  0.063244  0.157218  0.359391  0.000000  0.006468   0.066094       79.706246       54.983483        77.510157         47.745949         56.646203          69.282974  70.733295   57.891709  64.312502  Spain Primera Division            RCD Espanyol\n",
       "70             club_Real Madrid  0.055333  0.048552  0.242421  0.008895  0.018871   0.045970       76.499878       31.284066        70.772737         68.860774         65.529877          65.198635  59.518894   66.529762  63.024328  Spain Primera Division             Real Madrid\n",
       "69              club_Real Betis  0.024710  0.055922  0.424391  0.002140  0.012304   0.083840       64.087678       32.891463        81.254149         52.826595         60.826080          72.884567  59.411097   62.179081  60.795089  Spain Primera Division              Real Betis\n",
       "68            club_RCD Mallorca  0.000977  0.091834  0.102545  0.000000  0.012612   0.106051       54.468118       40.723529        62.715911         47.745949         61.047155          77.392412  52.635853   62.061839  57.348846  Spain Primera Division            RCD Mallorca\n",
       "81              club_Sevilla FC  0.025476  0.055922  0.221568  0.000000  0.023001   0.013451       64.397854       32.891434        69.571598         47.745949         68.488231          58.598604  55.620295   58.277595  56.948945  Spain Primera Division              Sevilla FC\n",
       "..                          ...       ...       ...       ...       ...       ...        ...             ...             ...              ...               ...               ...                ...        ...         ...        ...                     ...                     ...\n",
       "96         club_West Ham United -0.065329 -0.000411 -0.025274 -0.001318 -0.000820  -0.054099       27.592683       20.605453        55.353563         44.616755         51.426052          44.888880  34.517233   46.977229  40.747231  English Premier League         West Ham United\n",
       "18  club_Brighton & Hove Albion -0.073050 -0.007629 -0.368933 -0.000000 -0.001372  -0.052932       24.463182       19.031131        35.558836         47.745949         51.030969          45.125647  26.351050   47.967521  37.159286  English Premier League  Brighton & Hove Albion\n",
       "56        club_Newcastle United -0.073136 -0.009587 -0.123237 -0.007101 -0.003831  -0.066593       24.428359       18.604284        49.710892         30.890788         49.269546          42.353088  30.914512   40.837807  35.876159  English Premier League        Newcastle United\n",
       "24          club_Crystal Palace -0.079612 -0.003805 -0.639170 -0.007251 -0.001766  -0.027649       21.803584       19.865117        19.993252         30.533734         50.748752          50.257037  20.553984   43.846508  32.200246  English Premier League          Crystal Palace\n",
       "8              club_Aston Villa -0.097596 -0.017283 -0.209804 -0.016957 -0.017323  -0.125454       14.514385       16.925695        44.724653          7.495359         39.605456          30.406669  25.388244   25.835828  25.612036  English Premier League             Aston Villa\n",
       "\n",
       "[98 rows x 18 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge all dataframes\n",
    "merge_dfs = [df_ovr_la_fi, df_ovr_rf_fi, df_ovr_xgb_fi,\n",
    "             df_value_la_fi, df_value_rf_fi, df_value_xgb_fi,\n",
    "             df_skill_la_fi, df_skill_rf_fi, df_skill_xgb_fi,\n",
    "             df_util_la_fi, df_util_rf_fi, df_util_xgb_fi,\n",
    "             df_injury_la_fi, df_injury_rf_fi, df_injury_xgb_fi]\n",
    "df_c_fi = functools.reduce(lambda  left,right: pd.merge(left,right,on=['variable']), merge_dfs)\n",
    "\n",
    "# rescale all feature importance\n",
    "targets = ['ovr', 'value']\n",
    "models = ['la', 'rf', 'xgb']\n",
    "all_pattern = []\n",
    "for target in targets:\n",
    "    for model in models:\n",
    "        all_pattern.append(target + '_' + model)\n",
    "\n",
    "rescale_scores = []\n",
    "for pattern in all_pattern:\n",
    "    df_c_fi[pattern + '_rescale'] = MinMaxScaler().fit_transform(df_c_fi[[pattern]]) * 100\n",
    "    rescale_scores.append(pattern + '_rescale')\n",
    "\n",
    "# mean of each section\n",
    "for target in targets:\n",
    "    section_scores = [x for x in rescale_scores if x.startswith(target)]\n",
    "    df_c_fi[target + '_mean'] = df_c_fi[section_scores].mean(axis=1)\n",
    "\n",
    "# calculate mean of all scores \n",
    "df_c_fi['all_mean'] = df_c_fi[rescale_scores].mean(axis=1)\n",
    "\n",
    "# merge league name\n",
    "df_c_fi = df_c_fi.merge(df_league_club, on='variable')\n",
    "df_c_fi = df_c_fi.sort_values(['league_name','all_mean'], ascending=False)\n",
    "df_c_fi.to_csv('data/df_c_fi.csv', index=False)\n",
    "df_c_fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do  \n",
    "- Overall score distribution for each club (kernel density by year, 4 row * 5 column, 5 different figures)\n",
    "- Change in overall score distribution for each club (same)\n",
    "- Visualization of yearly change in club score\n",
    "- Visualization of top 5 clubs in club score on map?\n",
    "- radar chart by league\n",
    "- radar chart for top 6 teams\n",
    "- distribution for each indicator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create the dataset for Part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(221, 146) (17770, 146) (17991, 145)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('data/df_fifa.csv')\n",
    "\n",
    "# filter 2020 and 5 clubs (this dataframe is going to be test set)\n",
    "select_clubs = ['FC Barcelona','FC Bayern München','Real Madrid','Paris Saint-Germain','Juventus','Manchester City','Liverpool']\n",
    "df_a_2020 = df[(df['year'] == 2020) & (df['club'].isin(select_clubs))]\n",
    "\n",
    "# filter everything from 2019 (this is going to be training set)\n",
    "df_a_2019 = df[df['year'] == 2019]\n",
    "\n",
    "# merge two dataframe\n",
    "df_a = pd.concat([df_a_2019, df_a_2020]).drop(['d_foot_left'], axis=1)\n",
    "print(df_a_2020.shape, df_a_2019.shape, df_a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17991, 995)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dummies for nationality, work rate, foot\n",
    "df_a_nationality = pd.get_dummies(df_a[['nationality']], prefix='d_nationality')\n",
    "df_a_workrate = pd.get_dummies(df_a[['work_rate']], prefix='d_workrate')\n",
    "df_a_foot = pd.get_dummies(df_a[['preferred_foot']], prefix='d_foot')\n",
    "df_a_club = pd.get_dummies(df_a[['club']], prefix='d_foot')\n",
    "\n",
    "# concate them into df_a\n",
    "df_a = pd.concat([df_a, df_a_nationality, df_a_workrate, df_a_foot, df_a_club], axis=1)\n",
    "df_a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a.to_csv('data/df_a.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drop columns, imputation, and train-test divide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17770, 979) (221, 979)\n"
     ]
    }
   ],
   "source": [
    "# drop unnecessary columns\n",
    "drop_vars = ['sofifa_id','short_name','dob','nationality','work_rate','body_type','team_position','loaned_from',\n",
    "            'preferred_foot','joined','contract_valid_until','league_name','main_position','team_jersey_number',\n",
    "            'club']\n",
    "df_a_all = df_a.drop(drop_vars, axis=1)\n",
    "\n",
    "# impute zero values for fieldplayers and goalkeeping ability\n",
    "# this is because goalkeeping ability is not available for field players and vice versa\n",
    "impute_vars = ['ab_pace','ab_shooting','ab_passing','ab_dribbling','ab_defending','ab_physic','release_clause_eur']\n",
    "impute_vars = impute_vars + [x for x in df_a_all.columns if x.startswith('ab_gk')]\n",
    "for var in impute_vars:\n",
    "    df_a_all[var] = df_a_all[var].fillna(0)\n",
    "    \n",
    "# impute mean value for ab_mentality_conposure\n",
    "# Composure is a Player Attribute in FIFA that determines a player's the state or feeling of being calm and \n",
    "# controlling their frustration in matches frustration. (from FIFAplay)\n",
    "df_a_all['ab_mentality_composure'].fillna(df_a_all['ab_mentality_composure'].mean(), inplace=True)\n",
    "\n",
    "# select players in the following clubs as test set\n",
    "df_a_te = df_a_all[df_a_all['year'] == 2020].drop(['year'], axis=1)\n",
    "df_a_tr = df_a_all[df_a_all['year'] == 2019].drop(['year'], axis=1)\n",
    "print(df_a_tr.shape, df_a_te.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign X and y\n",
    "X_tr = df_a_tr.drop(['overall'], axis=1)\n",
    "X_te = df_a_te.drop(['overall'], axis=1)\n",
    "y_tr = df_a_tr['overall']\n",
    "y_te = df_a_te['overall']\n",
    "\n",
    "# standardization\n",
    "scaler = StandardScaler().fit(X_tr)\n",
    "X_tr_stan = scaler.transform(X_tr)\n",
    "X_te_stan = scaler.transform(X_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [==================================================] 5/5 Done...\n",
      "\n",
      "18.67553714870375 0.1\n"
     ]
    }
   ],
   "source": [
    "# set parameters\n",
    "la_alphas = [1e-2, 1e-1, 1, 1e+1, 1e+2]\n",
    "\n",
    "# create empty lists to store errors\n",
    "la_tr_err, la_val_err = [],[]\n",
    "\n",
    "# run regression for each alpha\n",
    "for i,alpha in enumerate(la_alphas):\n",
    "    # update progressbar\n",
    "    progressbar(i, len(la_alphas))\n",
    "    \n",
    "    # perform cross-validation on the training data with 10 folds and get the mse_scores\n",
    "    lasso = Lasso(alpha=alpha, max_iter=10000)\n",
    "    scores = cross_validate(lasso, \n",
    "                            X_tr_stan, \n",
    "                            y_tr, \n",
    "                            cv=5, \n",
    "                            scoring='neg_mean_squared_error', \n",
    "                            return_train_score=True,\n",
    "                            n_jobs=-1)\n",
    "    \n",
    "    #Compute the train and validation MSE\n",
    "    la_tr_err.append(scores['train_score'].mean() * -1)\n",
    "    la_val_err.append(scores['test_score'].mean() * -1)\n",
    "\n",
    "# find the degree that returns the minimum validation error\n",
    "la_min_val_err = min(la_val_err)\n",
    "la_best_alpha = la_alphas[la_val_err.index(la_min_val_err)]\n",
    "print(la_min_val_err, la_best_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.945287764552433 7.987019287381537\n"
     ]
    }
   ],
   "source": [
    "lasso_best = Lasso(alpha=la_best_alpha, max_iter=10000)\n",
    "lasso_best.fit(X_tr_stan, y_tr)\n",
    "\n",
    "lasso_tr_pred = lasso_best.predict(X_tr_stan)\n",
    "lasso_te_pred = lasso_best.predict(X_te_stan)\n",
    "\n",
    "lasso_mse_tr = mean_squared_error(y_tr, lasso_tr_pred)\n",
    "lasso_mse_te = mean_squared_error(y_te, lasso_te_pred)\n",
    "print(lasso_mse_tr, lasso_mse_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest regression (m=p/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  4.0min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed: 133.1min\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-f04f822d7ed1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mrf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwarm_start\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_tr_stan\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mrf_gs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrf_params\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'neg_mean_squared_error'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mrf_gs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_tr_stan\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_tr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# extract best parameters and estimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cs109a\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     70\u001b[0m                           FutureWarning)\n\u001b[0;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cs109a\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    734\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    735\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 736\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    737\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    738\u001b[0m         \u001b[1;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cs109a\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1188\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cs109a\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    713\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 715\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    716\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    717\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cs109a\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1042\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1043\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1044\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cs109a\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    919\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    920\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 921\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    922\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    923\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cs109a\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    541\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cs109a\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    428\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 430\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    431\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cs109a\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    294\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    295\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 296\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "rf_trees = list(range(100, 450, 50))\n",
    "rf_depths = list(range(5, 16, 1))\n",
    " \n",
    "rf_params = {'n_estimators': rf_trees, \n",
    "             'max_depth': rf_depths}\n",
    " \n",
    "# grid search\n",
    "rf = RandomForestRegressor(warm_start=True,max_features=int(X_tr_stan.shape[1]/3),random_state=81)\n",
    "rf_gs = GridSearchCV(estimator=rf,param_grid=rf_params,scoring='neg_mean_squared_error',verbose=1,n_jobs=-1)\n",
    "rf_gs.fit(X_tr_stan, y_tr)\n",
    "\n",
    "# extract best parameters and estimator\n",
    "rf_best_param = rf_gs.best_params_\n",
    "rf_best_estimator = rf_gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple Random Forest regression\n",
    "rf_tr_pred = rf_best_estimator.predict(X_tr_stan)\n",
    "rf_te_pred = rf_best_estimator.predict(X_te_stan)\n",
    "\n",
    "rf_mse_tr = mean_squared_error(y_tr, rf_tr_pred)\n",
    "rf_mse_te = mean_squared_error(y_te, rf_te_pred)\n",
    "print(rf_mse_tr, rf_mse_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "param_space = {'min_child_weight': hp.loguniform('min_child_weight', np.log(1), np.log(10)),\n",
    "               'max_depth': hp.quniform('max_depth', 3, 9, 1),\n",
    "               'subsample': hp.quniform('subsample', 0.6, 0.95, 0.05),\n",
    "               'colsample_bytree': hp.quniform('colsample_bytree', 0.6, 0.95, 0.05),\n",
    "               'gamma': hp.loguniform('gamma', np.log(1e-8), np.log(1.0)),\n",
    "               'reg_alpha': hp.loguniform('reg_alpha', np.log(1e-8), np.log(1.0)),\n",
    "               'reg_lambda': hp.loguniform('reg_lambda', np.log(1e-6), np.log(10.0))}\n",
    "\n",
    "# define score function\n",
    "def score(params):\n",
    "    xgb = XGBRegressor(random_state=81,\n",
    "                       min_child_weight=params['min_child_weight'],\n",
    "                       max_depth=int(params['max_depth']), \n",
    "                       subsample=params['subsample'], \n",
    "                       colsample_bytree=params['colsample_bytree'],\n",
    "                       gamma=params['gamma'], \n",
    "                       reg_alpha=params['reg_alpha'], \n",
    "                       reg_lambda=params['reg_lambda'])\n",
    "    scores = cross_validate(xgb, X_tr_stan, y_tr, \n",
    "                            cv=5, \n",
    "                            scoring='neg_mean_squared_error', \n",
    "                            n_jobs=-1)\n",
    "    return -1 * scores['test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 45%|█████████████████████▌                          | 45/100 [37:57<45:18, 49.43s/trial, best loss: 9.486956623152892]"
     ]
    }
   ],
   "source": [
    "# run gridsearch and find best parameters\n",
    "max_evals = 100\n",
    "trials = Trials()\n",
    "history = []\n",
    "rstate = np.random.RandomState(81)\n",
    "best_params = fmin(score, param_space, algo=tpe.suggest, trials=trials, max_evals=max_evals, rstate=rstate)\n",
    "\n",
    "# refit with the best parameters\n",
    "xgb_best = XGBRegressor(random_state=81,\n",
    "                        min_child_weight=best_params['min_child_weight'],\n",
    "                        max_depth=int(best_params['max_depth']), \n",
    "                        subsample=best_params['subsample'], \n",
    "                        colsample_bytree=best_params['colsample_bytree'],\n",
    "                        gamma=best_params['gamma'], \n",
    "                        reg_alpha=best_params['reg_alpha'], \n",
    "                        reg_lambda=best_params['reg_lambda'])\n",
    "xgb_best.fit(X_tr_stan, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_tr_pred = xgb_best.predict(X_tr_stan)\n",
    "xgb_te_pred = xgb_best.predict(X_te_stan)\n",
    "\n",
    "xgb_mse_tr = mean_squared_error(y_tr, xgb_tr_pred)\n",
    "xgb_mse_te = mean_squared_error(y_te, xgb_te_pred)\n",
    "print(xgb_mse_tr, xgb_mse_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
