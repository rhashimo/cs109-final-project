{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "blockquote { background: #AEDE94; }\n",
       "h1 { \n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "h2 { \n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "\tbackground-color: #ffcccc;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "div.exercise-r {\n",
       "\tbackground-color: #fce8e8;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "\n",
       "span.sub-q {\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "div.theme {\n",
       "\tbackground-color: #DDDDDD;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 18pt;\n",
       "}\n",
       "div.gc { \n",
       "\tbackground-color: #AEDE94;\n",
       "\tborder-color: #E9967A; \t \n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 12pt;\n",
       "}\n",
       "p.q1 { \n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RUN THIS CELL \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\n",
    "    \"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\"\n",
    ").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import functools\n",
    "import pickle\n",
    "\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(style='whitegrid')\n",
    "pd.set_option('display.width', 1500)\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a progressbar function\n",
    "def progressbar(n_step, n_total):\n",
    "    \"\"\"Prints self-updating progress bar to stdout to track for-loop progress\n",
    "    \n",
    "    There are entire 3rd-party libraries dedicated to custom progress-bars.\n",
    "    A simple function like this is often more than enough to get the job done.\n",
    "    \n",
    "    :param n_total: total number of expected for-loop iterations\n",
    "    :type n_total: int\n",
    "    :param n_step: current iteration number, starting at 0\n",
    "    :type n_step: int\n",
    "\n",
    "    .. example::\n",
    "    \n",
    "        for i in range(n_iterations):\n",
    "            progressbar(i, n_iterations)\n",
    "            \n",
    "    .. source:\n",
    "    \n",
    "        This function is a simplified version of code found here:\n",
    "        https://stackoverflow.com/questions/3160699/python-progress-bar/15860757#15860757\n",
    "    \"\"\"\n",
    "    n_step = n_step + 1\n",
    "    barlen = 50\n",
    "    progress = n_step / n_total\n",
    "    block = int(round(barlen * progress))\n",
    "    status = \"\"\n",
    "    if n_step == n_total:\n",
    "        status = \"Done...\\r\\n\\n\"\n",
    "    text = \"\\r [{0}] {1}/{2} {3}\".format(\n",
    "        \"=\" * block + \"-\" * (barlen - block),\n",
    "        n_step,\n",
    "        n_total,\n",
    "        status,\n",
    "    )\n",
    "    sys.stdout.write(text)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create the dataset for Part A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(221, 146) (17770, 146) (17991, 145)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('data/df_fifa.csv')\n",
    "\n",
    "# filter 2020 and 5 clubs (this dataframe is going to be test set)\n",
    "select_clubs = ['FC Barcelona','FC Bayern München','Real Madrid','Paris Saint-Germain','Juventus','Manchester City','Liverpool']\n",
    "df_a_2020 = df[(df['year'] == 2020) & (df['club'].isin(select_clubs))]\n",
    "\n",
    "# filter everything from 2019 (this is going to be training set)\n",
    "df_a_2019 = df[df['year'] == 2019]\n",
    "\n",
    "# merge two dataframe\n",
    "df_a = pd.concat([df_a_2019, df_a_2020]).drop(['d_foot_left'], axis=1)\n",
    "print(df_a_2020.shape, df_a_2019.shape, df_a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17991, 995)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dummies for nationality, work rate, foot\n",
    "df_a_nationality = pd.get_dummies(df_a[['nationality']], prefix='d_nationality')\n",
    "df_a_workrate = pd.get_dummies(df_a[['work_rate']], prefix='d_workrate')\n",
    "df_a_foot = pd.get_dummies(df_a[['preferred_foot']], prefix='d_foot')\n",
    "df_a_club = pd.get_dummies(df_a[['club']], prefix='d_foot')\n",
    "\n",
    "# concate them into df_a\n",
    "df_a = pd.concat([df_a, df_a_nationality, df_a_workrate, df_a_foot, df_a_club], axis=1)\n",
    "df_a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a.to_csv('data/df_a.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drop columns, imputation, and train-test divide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17770, 979) (221, 979)\n"
     ]
    }
   ],
   "source": [
    "# drop unnecessary columns\n",
    "drop_vars = ['sofifa_id','short_name','dob','nationality','work_rate','body_type','team_position','loaned_from',\n",
    "            'preferred_foot','joined','contract_valid_until','league_name','main_position','team_jersey_number',\n",
    "            'club']\n",
    "df_a_all = df_a.drop(drop_vars, axis=1)\n",
    "\n",
    "# impute zero values for fieldplayers and goalkeeping ability\n",
    "# this is because goalkeeping ability is not available for field players and vice versa\n",
    "impute_vars = ['ab_pace','ab_shooting','ab_passing','ab_dribbling','ab_defending','ab_physic','release_clause_eur']\n",
    "impute_vars = impute_vars + [x for x in df_a_all.columns if x.startswith('ab_gk')]\n",
    "for var in impute_vars:\n",
    "    df_a_all[var] = df_a_all[var].fillna(0)\n",
    "    \n",
    "# impute mean value for ab_mentality_conposure\n",
    "# Composure is a Player Attribute in FIFA that determines a player's the state or feeling of being calm and \n",
    "# controlling their frustration in matches frustration. (from FIFAplay)\n",
    "df_a_all['ab_mentality_composure'].fillna(df_a_all['ab_mentality_composure'].mean(), inplace=True)\n",
    "\n",
    "# select players in the following clubs as test set\n",
    "df_a_te = df_a_all[df_a_all['year'] == 2020].drop(['year'], axis=1)\n",
    "df_a_tr = df_a_all[df_a_all['year'] == 2019].drop(['year'], axis=1)\n",
    "print(df_a_tr.shape, df_a_te.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign X and y\n",
    "X_tr = df_a_tr.drop(['overall'], axis=1)\n",
    "X_te = df_a_te.drop(['overall'], axis=1)\n",
    "y_tr = df_a_tr['overall']\n",
    "y_te = df_a_te['overall']\n",
    "\n",
    "# standardization\n",
    "scaler = StandardScaler().fit(X_tr)\n",
    "X_tr_stan = scaler.transform(X_tr)\n",
    "X_te_stan = scaler.transform(X_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [==================================================] 5/5 Done...\n",
      "\n",
      "18.675537148703818 0.1\n"
     ]
    }
   ],
   "source": [
    "# set parameters\n",
    "la_alphas = [1e-2, 1e-1, 1, 1e+1, 1e+2]\n",
    "\n",
    "# create empty lists to store errors\n",
    "la_tr_err, la_val_err = [],[]\n",
    "\n",
    "# run regression for each alpha\n",
    "for i,alpha in enumerate(la_alphas):\n",
    "    # update progressbar\n",
    "    progressbar(i, len(la_alphas))\n",
    "    \n",
    "    # perform cross-validation on the training data with 10 folds and get the mse_scores\n",
    "    lasso = Lasso(alpha=alpha, max_iter=10000)\n",
    "    scores = cross_validate(lasso, \n",
    "                            X_tr_stan, \n",
    "                            y_tr, \n",
    "                            cv=5, \n",
    "                            scoring='neg_mean_squared_error', \n",
    "                            return_train_score=True,\n",
    "                            n_jobs=-1)\n",
    "    \n",
    "    #Compute the train and validation MSE\n",
    "    la_tr_err.append(scores['train_score'].mean() * -1)\n",
    "    la_val_err.append(scores['test_score'].mean() * -1)\n",
    "\n",
    "# find the degree that returns the minimum validation error\n",
    "la_min_val_err = min(la_val_err)\n",
    "la_best_alpha = la_alphas[la_val_err.index(la_min_val_err)]\n",
    "print(la_min_val_err, la_best_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.945287764552433 7.987019287381537\n"
     ]
    }
   ],
   "source": [
    "lasso_best = Lasso(alpha=la_best_alpha, max_iter=10000)\n",
    "lasso_best.fit(X_tr_stan, y_tr)\n",
    "\n",
    "lasso_tr_pred = lasso_best.predict(X_tr_stan)\n",
    "lasso_te_pred = lasso_best.predict(X_te_stan)\n",
    "\n",
    "lasso_mse_tr = mean_squared_error(y_tr, lasso_tr_pred)\n",
    "lasso_mse_te = mean_squared_error(y_te, lasso_te_pred)\n",
    "print(lasso_mse_tr, lasso_mse_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename = 'models/partA_lasso_best.sav'\n",
    "pickle.dump(lasso_best, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest regression (m=p/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 77 candidates, totalling 385 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  7.6min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed: 46.9min\n",
      "[Parallel(n_jobs=-1)]: Done 385 out of 385 | elapsed: 113.8min finished\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "rf_trees = list(range(100, 450, 50))\n",
    "rf_depths = list(range(5, 16, 1))\n",
    " \n",
    "rf_params = {'n_estimators': rf_trees, \n",
    "             'max_depth': rf_depths}\n",
    " \n",
    "# grid search\n",
    "rf = RandomForestRegressor(warm_start=True,max_features=int(X_tr_stan.shape[1]/3),random_state=81)\n",
    "rf_gs = GridSearchCV(estimator=rf,param_grid=rf_params,scoring='neg_mean_squared_error',verbose=1,n_jobs=-1)\n",
    "rf_gs.fit(X_tr_stan, y_tr)\n",
    "\n",
    "# extract best parameters and estimator\n",
    "rf_best_param = rf_gs.best_params_\n",
    "rf_best_estimator = rf_gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.102711098021617 0.722307640037196\n"
     ]
    }
   ],
   "source": [
    "# simple Random Forest regression\n",
    "rf_tr_pred = rf_best_estimator.predict(X_tr_stan)\n",
    "rf_te_pred = rf_best_estimator.predict(X_te_stan)\n",
    "\n",
    "rf_mse_tr = mean_squared_error(y_tr, rf_tr_pred)\n",
    "rf_mse_te = mean_squared_error(y_te, rf_te_pred)\n",
    "print(rf_mse_tr, rf_mse_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename = 'models/partA_rf_best.sav'\n",
    "pickle.dump(rf_best_estimator, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "param_space = {'min_child_weight': hp.loguniform('min_child_weight', np.log(1), np.log(10)),\n",
    "               'max_depth': hp.quniform('max_depth', 3, 9, 1),\n",
    "               'subsample': hp.quniform('subsample', 0.6, 0.95, 0.05),\n",
    "               'colsample_bytree': hp.quniform('colsample_bytree', 0.6, 0.95, 0.05),\n",
    "               'gamma': hp.loguniform('gamma', np.log(1e-8), np.log(1.0)),\n",
    "               'reg_alpha': hp.loguniform('reg_alpha', np.log(1e-8), np.log(1.0)),\n",
    "               'reg_lambda': hp.loguniform('reg_lambda', np.log(1e-6), np.log(10.0))}\n",
    "\n",
    "# define score function\n",
    "def score(params):\n",
    "    xgb = XGBRegressor(random_state=81,\n",
    "                       min_child_weight=params['min_child_weight'],\n",
    "                       max_depth=int(params['max_depth']), \n",
    "                       subsample=params['subsample'], \n",
    "                       colsample_bytree=params['colsample_bytree'],\n",
    "                       gamma=params['gamma'], \n",
    "                       reg_alpha=params['reg_alpha'], \n",
    "                       reg_lambda=params['reg_lambda'])\n",
    "    scores = cross_validate(xgb, X_tr_stan, y_tr, \n",
    "                            cv=5, \n",
    "                            scoring='neg_mean_squared_error', \n",
    "                            n_jobs=-1)\n",
    "    return -1 * scores['test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 100/100 [1:12:52<00:00, 43.72s/trial, best loss: 8.928813504932943]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=0.9500000000000001,\n",
       "             gamma=8.055721302386868e-06, gpu_id=-1, importance_type='gain',\n",
       "             interaction_constraints='', learning_rate=0.300000012,\n",
       "             max_delta_step=0, max_depth=3, min_child_weight=1.2274661445410096,\n",
       "             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,\n",
       "             num_parallel_tree=1, random_state=81,\n",
       "             reg_alpha=0.28671193975173354, reg_lambda=0.00650933174060852,\n",
       "             scale_pos_weight=1, subsample=0.7000000000000001,\n",
       "             tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run gridsearch and find best parameters\n",
    "max_evals = 100\n",
    "trials = Trials()\n",
    "history = []\n",
    "rstate = np.random.RandomState(81)\n",
    "best_params = fmin(score, param_space, algo=tpe.suggest, trials=trials, max_evals=max_evals, rstate=rstate)\n",
    "\n",
    "# refit with the best parameters\n",
    "xgb_best = XGBRegressor(random_state=81,\n",
    "                        min_child_weight=best_params['min_child_weight'],\n",
    "                        max_depth=int(best_params['max_depth']), \n",
    "                        subsample=best_params['subsample'], \n",
    "                        colsample_bytree=best_params['colsample_bytree'],\n",
    "                        gamma=best_params['gamma'], \n",
    "                        reg_alpha=best_params['reg_alpha'], \n",
    "                        reg_lambda=best_params['reg_lambda'])\n",
    "xgb_best.fit(X_tr_stan, y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4516870865917849 0.6840630820922725\n"
     ]
    }
   ],
   "source": [
    "xgb_tr_pred = xgb_best.predict(X_tr_stan)\n",
    "xgb_te_pred = xgb_best.predict(X_te_stan)\n",
    "\n",
    "xgb_mse_tr = mean_squared_error(y_tr, xgb_tr_pred)\n",
    "xgb_mse_te = mean_squared_error(y_te, xgb_te_pred)\n",
    "print(xgb_mse_tr, xgb_mse_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model to disk\n",
    "filename = 'models/partA_xgb_best.sav'\n",
    "pickle.dump(xgb_best, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create the dataset for Part C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ryuic\\Anaconda3\\envs\\cs109a\\lib\\site-packages\\pandas\\core\\series.py:726: RuntimeWarning: divide by zero encountered in log\n",
      "  result = getattr(ufunc, method)(*inputs, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('data/df_fifa.csv')\n",
    "\n",
    "# select players in the following clubs as test set\n",
    "df_c = df[df['d_div1_league']==1]\n",
    "\n",
    "# extract league and club\n",
    "df_league_club = df_c.copy()\n",
    "df_league_club = df_league_club[['league_name','club']].drop_duplicates()\n",
    "df_league_club['variable'] = 'club_' + df_league_club['club']\n",
    "\n",
    "# create lead variables for target variables (values in the following year)\n",
    "target_vars = ['overall','value_eur','skill_moves','d_multiple_position','d_trait_Injury_Prone']\n",
    "added_vars = []\n",
    "\n",
    "df_c = df_c.sort_values(['sofifa_id','year'])\n",
    "for i, target_var in enumerate(target_vars):\n",
    "    # insert lead variables next to original ones\n",
    "    col_index = list(df_c.columns).index(target_var) + 1\n",
    "    next_var = target_var + '_next'\n",
    "    diff_var = target_var + '_diff'\n",
    "    df_c.insert(col_index, next_var, df_c.groupby(['sofifa_id'])[target_var].shift(-1))\n",
    "    \n",
    "    # calculate change to the following years\n",
    "    # for value, calculate percentage change as distribution is highly skewed\n",
    "    if target_var == 'value_eur':\n",
    "        df_c.insert(col_index + 1, diff_var, np.log(df_c[next_var]) - np.log(df_c[target_var]))\n",
    "    else:\n",
    "        df_c.insert(col_index + 1, diff_var, df_c[next_var] - df_c[target_var])\n",
    "        \n",
    "    added_vars.append(next_var)\n",
    "    added_vars.append(diff_var)\n",
    "\n",
    "# create dummies for club, nationality, \n",
    "df_c_dummies = pd.get_dummies(df_c[['nationality','club','work_rate','league_name']])\n",
    "df_c = pd.concat([df_c, df_c_dummies], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### drop columns, imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_c.to_csv('data/df_c.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unnecessary columns\n",
    "drop_vars = ['sofifa_id','short_name','dob','nationality','club','wage_eur','preferred_foot','work_rate','body_type',\n",
    "             'team_position','team_jersey_number','joined','contract_valid_until','league_name','release_clause_eur',\n",
    "             'loaned_from','main_position','year']\n",
    "df_c_all = df_c.drop(drop_vars, axis=1)\n",
    "\n",
    "# impute zero values for fieldplayers and goalkeeping ability\n",
    "# this is because goalkeeping ability is not available for field players and vice versa\n",
    "impute_vars = ['ab_pace','ab_shooting','ab_passing','ab_dribbling','ab_defending','ab_physic'] + [x for x in df_c_all.columns if x.startswith('ab_gk')]\n",
    "for var in impute_vars:\n",
    "    df_c_all[var] = df_c_all[var].fillna(0)\n",
    "    \n",
    "# impute mean value for ab_mentality_conposure\n",
    "# Composure is a Player Attribute in FIFA that determines a player's the state or feeling of being calm and \n",
    "# controlling their frustration in matches frustration. (from FIFAplay)\n",
    "df_c_all['ab_mentality_composure'].fillna(df_c_all['ab_mentality_composure'].mean(), inplace=True)\n",
    "\n",
    "# save to csv\n",
    "df_c_all.to_csv('data/df_c_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic idea  \n",
    "- We define a club's ability to increase a player stats and value as the club's performance when we control the player's basic characteristics, e.g. age, reputation, skills, etc. \n",
    "- Given this, we regress players' overall score (annual change) on all these characteristics and club dummies, and identify the feature importance of a club dummy as the club's performance when controlling other factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter data, standardization, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10517, 361)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate a dataset for overall score regression and drop NA\n",
    "section_target = 'overall'\n",
    "added_vars_temp = [x for x in added_vars if not x == section_target + '_diff']\n",
    "df_c_ovr = df_c_all.copy().drop(added_vars_temp, axis=1)\n",
    "df_c_ovr = df_c_ovr.drop('value_eur', axis=1)\n",
    "df_c_ovr = df_c_ovr.dropna()\n",
    "\n",
    "# save to csv\n",
    "df_c_ovr.to_csv('data/df_c_ovr.csv', index=False)\n",
    "df_c_ovr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign X and y\n",
    "X = df_c_ovr.drop([section_target, section_target + '_diff'], axis=1)\n",
    "y = df_c_ovr[section_target + '_diff']\n",
    "\n",
    "# standardization\n",
    "scaler = StandardScaler().fit(X)\n",
    "X_stan = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [==================================================] 4/4 Done...\n",
      "\n",
      "6.743067001152627 0.01\n"
     ]
    }
   ],
   "source": [
    "# grid search\n",
    "# set parameters\n",
    "la_alphas = [1e-2, 1e-1, 1, 1e+1]\n",
    "\n",
    "# create empty lists to store errors\n",
    "la_tr_err, la_val_err = [],[]\n",
    "\n",
    "# run regression for each alpha\n",
    "for i,alpha in enumerate(la_alphas):\n",
    "    # update progressbar\n",
    "    progressbar(i, len(la_alphas))\n",
    "    \n",
    "    # perform cross-validation on the training data with 10 folds and get the mse_scores\n",
    "    lasso = Lasso(alpha=alpha, max_iter=10000)\n",
    "    scores = cross_validate(lasso, X_stan, y, \n",
    "                            cv=5, \n",
    "                            scoring='neg_mean_squared_error', \n",
    "                            return_train_score=True,\n",
    "                            n_jobs=-1)\n",
    "    \n",
    "    #Compute the train and validation MSE\n",
    "    la_tr_err.append(scores['train_score'].mean() * -1)\n",
    "    la_val_err.append(scores['test_score'].mean() * -1)\n",
    "\n",
    "# find the degree that returns the minimum validation error\n",
    "la_min_val_err = min(la_val_err)\n",
    "la_best_alpha = la_alphas[la_val_err.index(la_min_val_err)]\n",
    "print(la_min_val_err, la_best_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=0.01, max_iter=10000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# refit Lasso using best alpha\n",
    "la_best = Lasso(alpha=la_best_alpha, max_iter=10000)\n",
    "la_best.fit(X_stan, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store coefficients into dataframe\n",
    "df_ovr_la_fi = pd.DataFrame({'variable':X.columns,\n",
    "                             'ovr_la':la_best.coef_})\n",
    "df_ovr_la_fi = df_ovr_la_fi[df_ovr_la_fi['variable'].str.startswith('club')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest (m=p/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 77 candidates, totalling 385 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed: 10.4min\n",
      "[Parallel(n_jobs=-1)]: Done 385 out of 385 | elapsed: 26.6min finished\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "rf_trees = list(range(100, 450, 50))\n",
    "rf_depths = list(range(5, 16, 1))\n",
    " \n",
    "rf_params = {'n_estimators': rf_trees, \n",
    "             'max_depth': rf_depths}\n",
    " \n",
    "# grid search\n",
    "rf = RandomForestRegressor(warm_start=True,max_features=int(X_stan.shape[1]/3),random_state=0)\n",
    "rf_gs = GridSearchCV(estimator=rf,param_grid=rf_params,scoring='neg_mean_squared_error',verbose=1,n_jobs=-1)\n",
    "rf_gs.fit(X_stan, y)\n",
    "\n",
    "# extract best parameters and estimator\n",
    "rf_best_param = rf_gs.best_params_\n",
    "rf_best_estimator = rf_gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select club dummies\n",
    "clubs = [x for x in X.columns if x.startswith('club')]\n",
    "\n",
    "# compute feature importance\n",
    "# for each club, compute the difference of predicted values when the club dummy = 1 and 0\n",
    "# compute change in response variable for all players\n",
    "X_fi = pd.DataFrame(data=X_stan,\n",
    "                    columns=X.columns)\n",
    "\n",
    "# all club dummies = 0 (Since already standardized, impute the minimum value for each club)\n",
    "X_clubs_min = X_fi[clubs].min()\n",
    "X_clubs_max = X_fi[clubs].max()\n",
    "X_fi_zero = X_fi.copy()\n",
    "for club in clubs:\n",
    "    X_fi_zero[clubs] = X_clubs_min[club]\n",
    "\n",
    "rf_fi = []\n",
    "for club in clubs:\n",
    "    \n",
    "    # dummy for the seleted club = 1 (similarly, impute the max value for the club)\n",
    "    X_fi_one = X_fi_zero.copy()\n",
    "    X_fi_one[club] = X_clubs_max[club]\n",
    "    \n",
    "    # predict for each dataframe, take mean and take difference\n",
    "    pred_one = rf_best_estimator.predict(X_fi_one).mean()\n",
    "    pred_zero = rf_best_estimator.predict(X_fi_zero).mean()\n",
    "    pred_diff = pred_one - pred_zero\n",
    "    \n",
    "    # append\n",
    "    rf_fi.append(pred_diff)\n",
    "\n",
    "# store coefficients into dataframe\n",
    "df_ovr_rf_fi = pd.DataFrame({'variable':clubs,\n",
    "                             'ovr_rf':rf_fi})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "param_space = {'min_child_weight': hp.loguniform('min_child_weight', np.log(1), np.log(10)),\n",
    "               'max_depth': hp.quniform('max_depth', 3, 9, 1),\n",
    "               'subsample': hp.quniform('subsample', 0.6, 0.95, 0.05),\n",
    "               'colsample_bytree': hp.quniform('colsample_bytree', 0.6, 0.95, 0.05),\n",
    "               'gamma': hp.loguniform('gamma', np.log(1e-8), np.log(1.0)),\n",
    "               'reg_alpha': hp.loguniform('reg_alpha', np.log(1e-8), np.log(1.0)),\n",
    "               'reg_lambda': hp.loguniform('reg_lambda', np.log(1e-6), np.log(10.0))}\n",
    "\n",
    "# define score function\n",
    "def score(params):\n",
    "    xgb = XGBRegressor(random_state=81,\n",
    "                       min_child_weight=params['min_child_weight'],\n",
    "                       max_depth=int(params['max_depth']), \n",
    "                       subsample=params['subsample'], \n",
    "                       colsample_bytree=params['colsample_bytree'],\n",
    "                       gamma=params['gamma'], \n",
    "                       reg_alpha=params['reg_alpha'], \n",
    "                       reg_lambda=params['reg_lambda'])\n",
    "    scores = cross_validate(xgb, X_stan, y, \n",
    "                            cv=5, \n",
    "                            scoring='neg_mean_squared_error', \n",
    "                            n_jobs=-1)\n",
    "    return -1 * scores['test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████| 100/100 [15:22<00:00,  9.23s/trial, best loss: 6.667907084114356]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=0.65,\n",
       "             gamma=0.06737537763833035, gpu_id=-1, importance_type='gain',\n",
       "             interaction_constraints='', learning_rate=0.300000012,\n",
       "             max_delta_step=0, max_depth=3, min_child_weight=8.539019477920405,\n",
       "             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,\n",
       "             num_parallel_tree=1, random_state=81,\n",
       "             reg_alpha=0.16836423523146404, reg_lambda=9.890827461835658,\n",
       "             scale_pos_weight=1, subsample=0.7000000000000001,\n",
       "             tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run gridsearch and find best parameters\n",
    "max_evals = 100\n",
    "trials = Trials()\n",
    "history = []\n",
    "rstate = np.random.RandomState(81)\n",
    "best_params = fmin(score, param_space, algo=tpe.suggest, trials=trials, max_evals=max_evals, rstate=rstate)\n",
    "\n",
    "# refit with the best parameters\n",
    "xgb_best = XGBRegressor(random_state=81,\n",
    "                        min_child_weight=best_params['min_child_weight'],\n",
    "                        max_depth=int(best_params['max_depth']), \n",
    "                        subsample=best_params['subsample'], \n",
    "                        colsample_bytree=best_params['colsample_bytree'],\n",
    "                        gamma=best_params['gamma'], \n",
    "                        reg_alpha=best_params['reg_alpha'], \n",
    "                        reg_lambda=best_params['reg_lambda'])\n",
    "xgb_best.fit(X_stan, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute feature importance\n",
    "xgb_fi = []\n",
    "for club in clubs:\n",
    "    \n",
    "    # dummy for the seleted club = 1 (similarly, impute the max value for the club)\n",
    "    X_fi_one = X_fi_zero.copy()\n",
    "    X_fi_one[club] = X_clubs_max[club]\n",
    "    \n",
    "    # predict for each dataframe, take mean and take difference\n",
    "    pred_one = xgb_best.predict(X_fi_one.to_numpy()).mean()\n",
    "    pred_zero = xgb_best.predict(X_fi_zero.to_numpy()).mean()\n",
    "    pred_diff = pred_one - pred_zero\n",
    "    \n",
    "    # append\n",
    "    xgb_fi.append(pred_diff)\n",
    "\n",
    "# store coefficients into dataframe\n",
    "df_ovr_xgb_fi = pd.DataFrame({'variable':clubs,\n",
    "                              'ovr_xgb':xgb_fi})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Market value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter data, standardization, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8432, 362)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate a dataset for market value regression and drop NA\n",
    "section_target = 'value_eur'\n",
    "added_vars_temp = [x for x in added_vars if not x == section_target + '_diff']\n",
    "df_c_value = df_c_all.copy().drop(added_vars_temp, axis=1)\n",
    "df_c_value = df_c_value.replace([np.inf, -np.inf], np.nan)\n",
    "df_c_value = df_c_value.dropna()\n",
    "\n",
    "# save to csv\n",
    "df_c_value.to_csv('data/df_c_value.csv', index=False)\n",
    "df_c_value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign X and y\n",
    "X = df_c_value.drop([section_target, section_target + '_diff'], axis=1)\n",
    "y = df_c_value[section_target + '_diff']\n",
    "\n",
    "# standardization\n",
    "scaler = StandardScaler().fit(X)\n",
    "X_stan = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [==================================================] 4/4 Done...\n",
      "\n",
      "0.3110163303647024 0.01\n"
     ]
    }
   ],
   "source": [
    "# grid search\n",
    "# set parameters\n",
    "la_alphas = [1e-2, 1e-1, 1, 1e+1]\n",
    "\n",
    "# create empty lists to store errors\n",
    "la_tr_err, la_val_err = [],[]\n",
    "\n",
    "# run regression for each alpha\n",
    "for i,alpha in enumerate(la_alphas):\n",
    "    # update progressbar\n",
    "    progressbar(i, len(la_alphas))\n",
    "    \n",
    "    # perform cross-validation on the training data with 10 folds and get the mse_scores\n",
    "    lasso = Lasso(alpha=alpha, max_iter=10000)\n",
    "    scores = cross_validate(lasso, X_stan, y, \n",
    "                            cv=5, \n",
    "                            scoring='neg_mean_squared_error', \n",
    "                            return_train_score=True,\n",
    "                            n_jobs=-1)\n",
    "    \n",
    "    #Compute the train and validation MSE\n",
    "    la_tr_err.append(scores['train_score'].mean() * -1)\n",
    "    la_val_err.append(scores['test_score'].mean() * -1)\n",
    "\n",
    "# find the degree that returns the minimum validation error\n",
    "la_min_val_err = min(la_val_err)\n",
    "la_best_alpha = la_alphas[la_val_err.index(la_min_val_err)]\n",
    "print(la_min_val_err, la_best_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=0.01, max_iter=10000)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# refit Lasso using best alpha\n",
    "la_best = Lasso(alpha=la_best_alpha, max_iter=10000)\n",
    "la_best.fit(X_stan, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store coefficients into dataframe\n",
    "df_value_la_fi = pd.DataFrame({'variable':X.columns,\n",
    "                               'value_la':la_best.coef_})\n",
    "df_value_la_fi = df_value_la_fi[df_value_la_fi['variable'].str.startswith('club')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest (m=p/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 77 candidates, totalling 385 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  8.0min\n",
      "[Parallel(n_jobs=-1)]: Done 385 out of 385 | elapsed: 21.2min finished\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "rf_trees = list(range(100, 450, 50))\n",
    "rf_depths = list(range(5, 16, 1))\n",
    " \n",
    "rf_params = {'n_estimators': rf_trees, \n",
    "             'max_depth': rf_depths}\n",
    " \n",
    "# grid search\n",
    "rf = RandomForestRegressor(warm_start=True,max_features=int(X_stan.shape[1]/3),random_state=0)\n",
    "rf_gs = GridSearchCV(estimator=rf,param_grid=rf_params,scoring='neg_mean_squared_error',verbose=1,n_jobs=-1)\n",
    "rf_gs.fit(X_stan, y)\n",
    "\n",
    "# extract best parameters and estimator\n",
    "rf_best_param = rf_gs.best_params_\n",
    "rf_best_estimator = rf_gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select club dummies\n",
    "clubs = [x for x in X.columns if x.startswith('club')]\n",
    "\n",
    "# compute feature importance\n",
    "# for each club, compute the difference of predicted values when the club dummy = 1 and 0\n",
    "# compute change in response variable for all players\n",
    "X_fi = pd.DataFrame(data=X_stan,\n",
    "                    columns=X.columns)\n",
    "\n",
    "# all club dummies = 0 (Since already standardized, impute the minimum value for each club)\n",
    "X_clubs_min = X_fi[clubs].min()\n",
    "X_clubs_max = X_fi[clubs].max()\n",
    "X_fi_zero = X_fi.copy()\n",
    "for club in clubs:\n",
    "    X_fi_zero[clubs] = X_clubs_min[club]\n",
    "\n",
    "rf_fi = []\n",
    "for club in clubs:\n",
    "    \n",
    "    # dummy for the seleted club = 1 (similarly, impute the max value for the club)\n",
    "    X_fi_one = X_fi_zero.copy()\n",
    "    X_fi_one[club] = X_clubs_max[club]\n",
    "    \n",
    "    # predict for each dataframe, take mean and take difference\n",
    "    pred_one = rf_best_estimator.predict(X_fi_one).mean()\n",
    "    pred_zero = rf_best_estimator.predict(X_fi_zero).mean()\n",
    "    pred_diff = pred_one - pred_zero\n",
    "    \n",
    "    # append\n",
    "    rf_fi.append(pred_diff)\n",
    "\n",
    "# store coefficients into dataframe\n",
    "df_value_rf_fi = pd.DataFrame({'variable':clubs,\n",
    "                               'value_rf':rf_fi})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "param_space = {'min_child_weight': hp.loguniform('min_child_weight', np.log(1), np.log(10)),\n",
    "               'max_depth': hp.quniform('max_depth', 3, 9, 1),\n",
    "               'subsample': hp.quniform('subsample', 0.6, 0.95, 0.05),\n",
    "               'colsample_bytree': hp.quniform('colsample_bytree', 0.6, 0.95, 0.05),\n",
    "               'gamma': hp.loguniform('gamma', np.log(1e-8), np.log(1.0)),\n",
    "               'reg_alpha': hp.loguniform('reg_alpha', np.log(1e-8), np.log(1.0)),\n",
    "               'reg_lambda': hp.loguniform('reg_lambda', np.log(1e-6), np.log(10.0))}\n",
    "\n",
    "# define score function\n",
    "def score(params):\n",
    "    xgb = XGBRegressor(random_state=81,\n",
    "                       min_child_weight=params['min_child_weight'],\n",
    "                       max_depth=int(params['max_depth']), \n",
    "                       subsample=params['subsample'], \n",
    "                       colsample_bytree=params['colsample_bytree'],\n",
    "                       gamma=params['gamma'], \n",
    "                       reg_alpha=params['reg_alpha'], \n",
    "                       reg_lambda=params['reg_lambda'])\n",
    "    scores = cross_validate(xgb, X_stan, y, \n",
    "                            cv=5, \n",
    "                            scoring='neg_mean_squared_error', \n",
    "                            n_jobs=-1)\n",
    "    return -1 * scores['test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████| 100/100 [13:33<00:00,  8.14s/trial, best loss: 0.3183672646921562]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=0.8500000000000001,\n",
       "             gamma=0.0023861216083917386, gpu_id=-1, importance_type='gain',\n",
       "             interaction_constraints='', learning_rate=0.300000012,\n",
       "             max_delta_step=0, max_depth=3, min_child_weight=1.9952861535592965,\n",
       "             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,\n",
       "             num_parallel_tree=1, random_state=81,\n",
       "             reg_alpha=5.628704955261183e-07, reg_lambda=9.81298801419108,\n",
       "             scale_pos_weight=1, subsample=0.8, tree_method='exact',\n",
       "             validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run gridsearch and find best parameters\n",
    "max_evals = 100\n",
    "trials = Trials()\n",
    "history = []\n",
    "rstate = np.random.RandomState(81)\n",
    "best_params = fmin(score, param_space, algo=tpe.suggest, trials=trials, max_evals=max_evals, rstate=rstate)\n",
    "\n",
    "# refit with the best parameters\n",
    "xgb_best = XGBRegressor(random_state=81,\n",
    "                        min_child_weight=best_params['min_child_weight'],\n",
    "                        max_depth=int(best_params['max_depth']), \n",
    "                        subsample=best_params['subsample'], \n",
    "                        colsample_bytree=best_params['colsample_bytree'],\n",
    "                        gamma=best_params['gamma'], \n",
    "                        reg_alpha=best_params['reg_alpha'], \n",
    "                        reg_lambda=best_params['reg_lambda'])\n",
    "xgb_best.fit(X_stan, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute feature importance\n",
    "xgb_fi = []\n",
    "for club in clubs:\n",
    "    \n",
    "    # dummy for the seleted club = 1 (similarly, impute the max value for the club)\n",
    "    X_fi_one = X_fi_zero.copy()\n",
    "    X_fi_one[club] = X_clubs_max[club]\n",
    "    \n",
    "    # predict for each dataframe, take mean and take difference\n",
    "    pred_one = xgb_best.predict(X_fi_one.to_numpy()).mean()\n",
    "    pred_zero = xgb_best.predict(X_fi_zero.to_numpy()).mean()\n",
    "    pred_diff = pred_one - pred_zero\n",
    "    \n",
    "    # append\n",
    "    xgb_fi.append(pred_diff)\n",
    "\n",
    "# store coefficients into dataframe\n",
    "df_value_xgb_fi = pd.DataFrame({'variable':clubs,\n",
    "                                'value_xgb':xgb_fi})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skill move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter data, standardization, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10517, 361)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate a dataset for overall score regression and drop NA\n",
    "section_target = 'skill_moves'\n",
    "added_vars_temp = [x for x in added_vars if not x == section_target + '_diff']\n",
    "df_c_skill = df_c_all.copy().drop(added_vars_temp, axis=1)\n",
    "df_c_skill = df_c_skill.drop('value_eur', axis=1)\n",
    "df_c_skill = df_c_skill.dropna()\n",
    "\n",
    "# save to csv\n",
    "df_c_skill.to_csv('data/df_c_skill.csv', index=False)\n",
    "df_c_skill.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign X and y\n",
    "X = df_c_skill.drop([section_target, section_target + '_diff'], axis=1)\n",
    "y = df_c_skill[section_target + '_diff']\n",
    "\n",
    "# standardization\n",
    "scaler = StandardScaler().fit(X)\n",
    "X_stan = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [==================================================] 4/4 Done...\n",
      "\n",
      "0.1033933988767747 0.01\n"
     ]
    }
   ],
   "source": [
    "# grid search\n",
    "# set parameters\n",
    "la_alphas = [1e-2, 1e-1, 1, 1e+1]\n",
    "\n",
    "# create empty lists to store errors\n",
    "la_tr_err, la_val_err = [],[]\n",
    "\n",
    "# run regression for each alpha\n",
    "for i,alpha in enumerate(la_alphas):\n",
    "    # update progressbar\n",
    "    progressbar(i, len(la_alphas))\n",
    "    \n",
    "    # perform cross-validation on the training data with 10 folds and get the mse_scores\n",
    "    lasso = Lasso(alpha=alpha, max_iter=10000)\n",
    "    scores = cross_validate(lasso, X_stan, y, \n",
    "                            cv=5, \n",
    "                            scoring='neg_mean_squared_error', \n",
    "                            return_train_score=True,\n",
    "                            n_jobs=-1)\n",
    "    \n",
    "    #Compute the train and validation MSE\n",
    "    la_tr_err.append(scores['train_score'].mean() * -1)\n",
    "    la_val_err.append(scores['test_score'].mean() * -1)\n",
    "\n",
    "# find the degree that returns the minimum validation error\n",
    "la_min_val_err = min(la_val_err)\n",
    "la_best_alpha = la_alphas[la_val_err.index(la_min_val_err)]\n",
    "print(la_min_val_err, la_best_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=0.01, max_iter=10000)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# refit Lasso using best alpha\n",
    "la_best = Lasso(alpha=la_best_alpha, max_iter=10000)\n",
    "la_best.fit(X_stan, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store coefficients into dataframe\n",
    "df_skill_la_fi = pd.DataFrame({'variable':X.columns,\n",
    "                               'skill_la':la_best.coef_})\n",
    "df_skill_la_fi = df_skill_la_fi[df_skill_la_fi['variable'].str.startswith('club')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest (m=p/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 77 candidates, totalling 385 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed: 10.7min\n",
      "[Parallel(n_jobs=-1)]: Done 385 out of 385 | elapsed: 27.5min finished\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "rf_trees = list(range(100, 450, 50))\n",
    "rf_depths = list(range(5, 16, 1))\n",
    " \n",
    "rf_params = {'n_estimators': rf_trees, \n",
    "             'max_depth': rf_depths}\n",
    " \n",
    "# grid search\n",
    "rf = RandomForestRegressor(warm_start=True,max_features=int(X_stan.shape[1]/3),random_state=0)\n",
    "rf_gs = GridSearchCV(estimator=rf,param_grid=rf_params,scoring='neg_mean_squared_error',verbose=1,n_jobs=-1)\n",
    "rf_gs.fit(X_stan, y)\n",
    "\n",
    "# extract best parameters and estimator\n",
    "rf_best_param = rf_gs.best_params_\n",
    "rf_best_estimator = rf_gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select club dummies\n",
    "clubs = [x for x in X.columns if x.startswith('club')]\n",
    "\n",
    "# compute feature importance\n",
    "# for each club, compute the difference of predicted values when the club dummy = 1 and 0\n",
    "# compute change in response variable for all players\n",
    "X_fi = pd.DataFrame(data=X_stan,\n",
    "                    columns=X.columns)\n",
    "\n",
    "# all club dummies = 0 (Since already standardized, impute the minimum value for each club)\n",
    "X_clubs_min = X_fi[clubs].min()\n",
    "X_clubs_max = X_fi[clubs].max()\n",
    "X_fi_zero = X_fi.copy()\n",
    "for club in clubs:\n",
    "    X_fi_zero[clubs] = X_clubs_min[club]\n",
    "\n",
    "rf_fi = []\n",
    "for club in clubs:\n",
    "    \n",
    "    # dummy for the seleted club = 1 (similarly, impute the max value for the club)\n",
    "    X_fi_one = X_fi_zero.copy()\n",
    "    X_fi_one[club] = X_clubs_max[club]\n",
    "    \n",
    "    # predict for each dataframe, take mean and take difference\n",
    "    pred_one = rf_best_estimator.predict(X_fi_one).mean()\n",
    "    pred_zero = rf_best_estimator.predict(X_fi_zero).mean()\n",
    "    pred_diff = pred_one - pred_zero\n",
    "    \n",
    "    # append\n",
    "    rf_fi.append(pred_diff)\n",
    "\n",
    "# store coefficients into dataframe\n",
    "df_skill_rf_fi = pd.DataFrame({'variable':clubs,\n",
    "                               'skill_rf':rf_fi})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "param_space = {'min_child_weight': hp.loguniform('min_child_weight', np.log(1), np.log(10)),\n",
    "               'max_depth': hp.quniform('max_depth', 3, 9, 1),\n",
    "               'subsample': hp.quniform('subsample', 0.6, 0.95, 0.05),\n",
    "               'colsample_bytree': hp.quniform('colsample_bytree', 0.6, 0.95, 0.05),\n",
    "               'gamma': hp.loguniform('gamma', np.log(1e-8), np.log(1.0)),\n",
    "               'reg_alpha': hp.loguniform('reg_alpha', np.log(1e-8), np.log(1.0)),\n",
    "               'reg_lambda': hp.loguniform('reg_lambda', np.log(1e-6), np.log(10.0))}\n",
    "\n",
    "# define score function\n",
    "def score(params):\n",
    "    xgb = XGBRegressor(random_state=81,\n",
    "                       min_child_weight=params['min_child_weight'],\n",
    "                       max_depth=int(params['max_depth']), \n",
    "                       subsample=params['subsample'], \n",
    "                       colsample_bytree=params['colsample_bytree'],\n",
    "                       gamma=params['gamma'], \n",
    "                       reg_alpha=params['reg_alpha'], \n",
    "                       reg_lambda=params['reg_lambda'])\n",
    "    scores = cross_validate(xgb, X_stan, y, \n",
    "                            cv=5, \n",
    "                            scoring='neg_mean_squared_error', \n",
    "                            n_jobs=-1)\n",
    "    return -1 * scores['test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 100/100 [16:11<00:00,  9.72s/trial, best loss: 0.09706565648331153]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=0.8,\n",
       "             gamma=0.00035067842933988325, gpu_id=-1, importance_type='gain',\n",
       "             interaction_constraints='', learning_rate=0.300000012,\n",
       "             max_delta_step=0, max_depth=3, min_child_weight=3.6529012108412164,\n",
       "             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,\n",
       "             num_parallel_tree=1, random_state=81,\n",
       "             reg_alpha=2.802043489398779e-05, reg_lambda=7.776596353600898,\n",
       "             scale_pos_weight=1, subsample=0.8, tree_method='exact',\n",
       "             validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run gridsearch and find best parameters\n",
    "max_evals = 100\n",
    "trials = Trials()\n",
    "history = []\n",
    "rstate = np.random.RandomState(81)\n",
    "best_params = fmin(score, param_space, algo=tpe.suggest, trials=trials, max_evals=max_evals, rstate=rstate)\n",
    "\n",
    "# refit with the best parameters\n",
    "xgb_best = XGBRegressor(random_state=81,\n",
    "                        min_child_weight=best_params['min_child_weight'],\n",
    "                        max_depth=int(best_params['max_depth']), \n",
    "                        subsample=best_params['subsample'], \n",
    "                        colsample_bytree=best_params['colsample_bytree'],\n",
    "                        gamma=best_params['gamma'], \n",
    "                        reg_alpha=best_params['reg_alpha'], \n",
    "                        reg_lambda=best_params['reg_lambda'])\n",
    "xgb_best.fit(X_stan, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute feature importance\n",
    "xgb_fi = []\n",
    "for club in clubs:\n",
    "    \n",
    "    # dummy for the seleted club = 1 (similarly, impute the max value for the club)\n",
    "    X_fi_one = X_fi_zero.copy()\n",
    "    X_fi_one[club] = X_clubs_max[club]\n",
    "    \n",
    "    # predict for each dataframe, take mean and take difference\n",
    "    pred_one = xgb_best.predict(X_fi_one.to_numpy()).mean()\n",
    "    pred_zero = xgb_best.predict(X_fi_zero.to_numpy()).mean()\n",
    "    pred_diff = pred_one - pred_zero\n",
    "    \n",
    "    # append\n",
    "    xgb_fi.append(pred_diff)\n",
    "\n",
    "# store coefficients into dataframe\n",
    "df_skill_xgb_fi = pd.DataFrame({'variable':clubs,\n",
    "                                'skill_xgb':xgb_fi})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter data, standardization, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10517, 361)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate a dataset for overall score regression and drop NA\n",
    "section_target = 'd_multiple_position'\n",
    "added_vars_temp = [x for x in added_vars if not x == section_target + '_diff']\n",
    "df_c_util = df_c_all.copy().drop(added_vars_temp, axis=1)\n",
    "df_c_util = df_c_util.drop('value_eur', axis=1)\n",
    "df_c_util = df_c_util.dropna()\n",
    "\n",
    "# save to csv\n",
    "df_c_util.to_csv('data/df_c_util.csv', index=False)\n",
    "df_c_util.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign X and y\n",
    "X = df_c_util.drop([section_target, section_target + '_diff'], axis=1)\n",
    "y = df_c_util[section_target + '_diff']\n",
    "\n",
    "# count # of observations by class\n",
    "# set the desired # of minority classes (up/down) at 20% of the majority class (stay)\n",
    "counter = y.value_counts().to_dict()\n",
    "for key in counter.keys():\n",
    "    if not key == 0.0:\n",
    "        counter[key] = int(counter[0.0] * 0.2)\n",
    "\n",
    "# upsampling using SMOTE. \n",
    "oversample = SMOTE(sampling_strategy = counter)\n",
    "X, y = oversample.fit_resample(X, y)\n",
    "counter_new = y.value_counts().to_dict()\n",
    "\n",
    "# standardization\n",
    "scaler = StandardScaler().fit(X)\n",
    "X_stan = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   1 tasks      | elapsed:  6.4min\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of  15 | elapsed:  6.4min remaining: 41.4min\n",
      "[Parallel(n_jobs=-1)]: Done   3 out of  15 | elapsed:  7.3min remaining: 29.3min\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of  15 | elapsed: 11.1min remaining: 30.4min\n"
     ]
    }
   ],
   "source": [
    "# grid search\n",
    "# set parameters\n",
    "Cs = [1e-3,1e-2,1e-1,1e0,1e1,1e2,1e3]\n",
    "cv = 5\n",
    "penalty = 'l1'\n",
    "solver = 'liblinear'\n",
    "\n",
    "# use logistic regression cv to find the best model \n",
    "log = LogisticRegressionCV(Cs=Cs, \n",
    "                           cv=cv, \n",
    "                           penalty=penalty, \n",
    "                           solver=solver,\n",
    "                           class_weight='balanced',\n",
    "                           n_jobs=-1,\n",
    "                           verbose=50)\n",
    "log.fit(X_stan,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select club dummies\n",
    "clubs = [x for x in X.columns if x.startswith('club')]\n",
    "\n",
    "# compute feature importance\n",
    "# for each club, compute the difference of predicted values when the club dummy = 1 and 0\n",
    "# compute change in response variable for all players\n",
    "X_fi = pd.DataFrame(data=X_stan,\n",
    "                    columns=X.columns)\n",
    "\n",
    "# all club dummies = 0 (Since already standardized, impute the minimum value for each club)\n",
    "X_clubs_min = X_fi[clubs].min()\n",
    "X_clubs_max = X_fi[clubs].max()\n",
    "X_fi_zero = X_fi.copy()\n",
    "for club in clubs:\n",
    "    X_fi_zero[clubs] = X_clubs_min[club]\n",
    "\n",
    "log_fi = []\n",
    "for club in clubs:\n",
    "    \n",
    "    # dummy for the seleted club = 1 (similarly, impute the max value for the club)\n",
    "    X_fi_one = X_fi_zero.copy()\n",
    "    X_fi_one[club] = X_clubs_max[club]\n",
    "    \n",
    "    # predict for each dataframe, take mean and take difference\n",
    "    pred_one = log.predict(X_fi_one).mean()\n",
    "    pred_zero = log.predict(X_fi_zero).mean()\n",
    "    pred_diff = pred_one - pred_zero\n",
    "    \n",
    "    # append\n",
    "    log_fi.append(pred_diff)\n",
    "\n",
    "# store coefficients into dataframe\n",
    "df_util_log_fi = pd.DataFrame({'variable':clubs,\n",
    "                               'util_log':log_fi})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "rf_trees = list(range(100, 450, 50))\n",
    "rf_depths = list(range(5, 16, 1))\n",
    " \n",
    "rf_params = {'n_estimators': rf_trees, \n",
    "             'max_depth': rf_depths}\n",
    " \n",
    "# grid search\n",
    "rf = RandomForestClassifier(max_features='auto',class_weight='balanced_subsample',random_state=0)\n",
    "rf_gs = GridSearchCV(estimator=rf,param_grid=rf_params,scoring='accuracy',verbose=1,n_jobs=-1)\n",
    "rf_gs.fit(X_stan, y)\n",
    "\n",
    "# extract best parameters and estimator\n",
    "rf_best_param = rf_gs.best_params_\n",
    "rf_best_estimator = rf_gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select club dummies\n",
    "clubs = [x for x in X.columns if x.startswith('club')]\n",
    "\n",
    "# compute feature importance\n",
    "# for each club, compute the difference of predicted values when the club dummy = 1 and 0\n",
    "# compute change in response variable for all players\n",
    "X_fi = pd.DataFrame(data=X_stan,\n",
    "                    columns=X.columns)\n",
    "\n",
    "# all club dummies = 0 (Since already standardized, impute the minimum value for each club)\n",
    "X_clubs_min = X_fi[clubs].min()\n",
    "X_clubs_max = X_fi[clubs].max()\n",
    "X_fi_zero = X_fi.copy()\n",
    "for club in clubs:\n",
    "    X_fi_zero[clubs] = X_clubs_min[club]\n",
    "\n",
    "rf_fi = []\n",
    "for club in clubs:\n",
    "    \n",
    "    # dummy for the seleted club = 1 (similarly, impute the max value for the club)\n",
    "    X_fi_one = X_fi_zero.copy()\n",
    "    X_fi_one[club] = X_clubs_max[club]\n",
    "    \n",
    "    # predict for each dataframe, take mean and take difference\n",
    "    pred_one = rf_best_estimator.predict(X_fi_one).mean()\n",
    "    pred_zero = rf_best_estimator.predict(X_fi_zero).mean()\n",
    "    pred_diff = pred_one - pred_zero\n",
    "    \n",
    "    # append\n",
    "    rf_fi.append(pred_diff)\n",
    "\n",
    "# store coefficients into dataframe\n",
    "df_util_rf_fi = pd.DataFrame({'variable':clubs,\n",
    "                              'util_rf':rf_fi})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "param_space = {'min_child_weight': hp.loguniform('min_child_weight', np.log(1), np.log(10)),\n",
    "               'max_depth': hp.quniform('max_depth', 3, 9, 1),\n",
    "               'subsample': hp.quniform('subsample', 0.6, 0.95, 0.05),\n",
    "               'colsample_bytree': hp.quniform('colsample_bytree', 0.6, 0.95, 0.05),\n",
    "               'gamma': hp.loguniform('gamma', np.log(1e-8), np.log(1.0)),\n",
    "               'reg_alpha': hp.loguniform('reg_alpha', np.log(1e-8), np.log(1.0)),\n",
    "               'reg_lambda': hp.loguniform('reg_lambda', np.log(1e-6), np.log(10.0))}\n",
    "\n",
    "# define score function\n",
    "def score(params):\n",
    "    xgb = XGBClassifier(random_state=81,\n",
    "                        min_child_weight=params['min_child_weight'],\n",
    "                        max_depth=int(params['max_depth']), \n",
    "                        subsample=params['subsample'], \n",
    "                        colsample_bytree=params['colsample_bytree'],\n",
    "                        gamma=params['gamma'], \n",
    "                        reg_alpha=params['reg_alpha'], \n",
    "                        reg_lambda=params['reg_lambda'])\n",
    "    scores = cross_validate(xgb, \n",
    "                            X=X_stan, \n",
    "                            y=y, \n",
    "                            cv=5, \n",
    "                            scoring='accuracy', \n",
    "                            n_jobs=-1)\n",
    "    return -1 * scores['test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run gridsearch and find best parameters\n",
    "max_evals = 100\n",
    "trials = Trials()\n",
    "history = []\n",
    "rstate = np.random.RandomState(81)\n",
    "best_params = fmin(score, param_space, algo=tpe.suggest, trials=trials, max_evals=max_evals, rstate=rstate)\n",
    "\n",
    "# refit with the best parameters\n",
    "xgb_best = XGBRegressor(random_state=81,\n",
    "                        min_child_weight=best_params['min_child_weight'],\n",
    "                        max_depth=int(best_params['max_depth']), \n",
    "                        subsample=best_params['subsample'], \n",
    "                        colsample_bytree=best_params['colsample_bytree'],\n",
    "                        gamma=best_params['gamma'], \n",
    "                        reg_alpha=best_params['reg_alpha'], \n",
    "                        reg_lambda=best_params['reg_lambda'])\n",
    "xgb_best.fit(X_stan, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute feature importance\n",
    "xgb_fi = []\n",
    "for club in clubs:\n",
    "    \n",
    "    # dummy for the seleted club = 1 (similarly, impute the max value for the club)\n",
    "    X_fi_one = X_fi_zero.copy()\n",
    "    X_fi_one[club] = X_clubs_max[club]\n",
    "    \n",
    "    # predict for each dataframe, take mean and take difference\n",
    "    pred_one = xgb_best.predict(X_fi_one.to_numpy()).mean()\n",
    "    pred_zero = xgb_best.predict(X_fi_zero.to_numpy()).mean()\n",
    "    pred_diff = pred_one - pred_zero\n",
    "    \n",
    "    # append\n",
    "    xgb_fi.append(pred_diff)\n",
    "\n",
    "# store coefficients into dataframe\n",
    "df_util_xgb_fi = pd.DataFrame({'variable':clubs,\n",
    "                              'util_xgb':xgb_fi})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Injury Proneness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter data, standardization, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10517, 361)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# separate a dataset for overall score regression and drop NA\n",
    "section_target = 'd_trait_Injury_Prone'\n",
    "added_vars_temp = [x for x in added_vars if not x == section_target + '_diff']\n",
    "df_c_injury = df_c_all.copy().drop(added_vars_temp, axis=1)\n",
    "df_c_injury = df_c_injury.drop('value_eur', axis=1)\n",
    "df_c_injury = df_c_injury.dropna()\n",
    "\n",
    "# save to csv\n",
    "df_c_injury.to_csv('data/df_c_injury.csv', index=False)\n",
    "df_c_injury.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign X and y\n",
    "X = df_c_injury.drop([section_target, section_target + '_diff'], axis=1)\n",
    "y = df_c_injury[section_target + '_diff']\n",
    "\n",
    "# standardization\n",
    "scaler = StandardScaler().fit(X)\n",
    "X_stan = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [==================================================] 4/4 Done...\n",
      "\n",
      "0.06269139096831858 0.01\n"
     ]
    }
   ],
   "source": [
    "# grid search\n",
    "# set parameters\n",
    "la_alphas = [1e-2, 1e-1, 1, 1e+1]\n",
    "\n",
    "# create empty lists to store errors\n",
    "la_tr_err, la_val_err = [],[]\n",
    "\n",
    "# run regression for each alpha\n",
    "for i,alpha in enumerate(la_alphas):\n",
    "    # update progressbar\n",
    "    progressbar(i, len(la_alphas))\n",
    "    \n",
    "    # perform cross-validation on the training data with 10 folds and get the mse_scores\n",
    "    lasso = Lasso(alpha=alpha, max_iter=10000)\n",
    "    scores = cross_validate(lasso, X_stan, y, \n",
    "                            cv=5, \n",
    "                            scoring='neg_mean_squared_error', \n",
    "                            return_train_score=True,\n",
    "                            n_jobs=-1)\n",
    "    \n",
    "    #Compute the train and validation MSE\n",
    "    la_tr_err.append(scores['train_score'].mean() * -1)\n",
    "    la_val_err.append(scores['test_score'].mean() * -1)\n",
    "\n",
    "# find the degree that returns the minimum validation error\n",
    "la_min_val_err = min(la_val_err)\n",
    "la_best_alpha = la_alphas[la_val_err.index(la_min_val_err)]\n",
    "print(la_min_val_err, la_best_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Lasso(alpha=0.01, max_iter=10000)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# refit Lasso using best alpha\n",
    "la_best = Lasso(alpha=la_best_alpha, max_iter=10000)\n",
    "la_best.fit(X_stan, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store coefficients into dataframe\n",
    "df_injury_la_fi = pd.DataFrame({'variable':X.columns,\n",
    "                                'injury_la':-1*(la_best.coef_)})\n",
    "df_injury_la_fi = df_injury_la_fi[df_injury_la_fi['variable'].str.startswith('club')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest (m=p/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 77 candidates, totalling 385 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed: 12.2min\n",
      "[Parallel(n_jobs=-1)]: Done 385 out of 385 | elapsed: 34.2min finished\n"
     ]
    }
   ],
   "source": [
    "# parameters\n",
    "rf_trees = list(range(100, 450, 50))\n",
    "rf_depths = list(range(5, 16, 1))\n",
    " \n",
    "rf_params = {'n_estimators': rf_trees, \n",
    "             'max_depth': rf_depths}\n",
    " \n",
    "# grid search\n",
    "rf = RandomForestRegressor(warm_start=True,max_features=int(X_stan.shape[1]/3),random_state=0)\n",
    "rf_gs = GridSearchCV(estimator=rf,param_grid=rf_params,scoring='neg_mean_squared_error',verbose=1,n_jobs=-1)\n",
    "rf_gs.fit(X_stan, y)\n",
    "\n",
    "# extract best parameters and estimator\n",
    "rf_best_param = rf_gs.best_params_\n",
    "rf_best_estimator = rf_gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select club dummies\n",
    "clubs = [x for x in X.columns if x.startswith('club')]\n",
    "\n",
    "# compute feature importance\n",
    "# for each club, compute the difference of predicted values when the club dummy = 1 and 0\n",
    "# compute change in response variable for all players\n",
    "X_fi = pd.DataFrame(data=X_stan,\n",
    "                    columns=X.columns)\n",
    "\n",
    "# all club dummies = 0 (Since already standardized, impute the minimum value for each club)\n",
    "X_clubs_min = X_fi[clubs].min()\n",
    "X_clubs_max = X_fi[clubs].max()\n",
    "X_fi_zero = X_fi.copy()\n",
    "for club in clubs:\n",
    "    X_fi_zero[clubs] = X_clubs_min[club]\n",
    "\n",
    "rf_fi = []\n",
    "for club in clubs:\n",
    "    \n",
    "    # dummy for the seleted club = 1 (similarly, impute the max value for the club)\n",
    "    X_fi_one = X_fi_zero.copy()\n",
    "    X_fi_one[club] = X_clubs_max[club]\n",
    "    \n",
    "    # predict for each dataframe, take mean and take difference\n",
    "    pred_one = rf_best_estimator.predict(X_fi_one).mean()\n",
    "    pred_zero = rf_best_estimator.predict(X_fi_zero).mean()\n",
    "    pred_diff = -1*(pred_one - pred_zero)\n",
    "    \n",
    "    # append\n",
    "    rf_fi.append(pred_diff)\n",
    "\n",
    "# store coefficients into dataframe\n",
    "df_injury_rf_fi = pd.DataFrame({'variable':clubs,\n",
    "                                'injury_rf':rf_fi})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set parameters\n",
    "param_space = {'min_child_weight': hp.loguniform('min_child_weight', np.log(1), np.log(10)),\n",
    "               'max_depth': hp.quniform('max_depth', 3, 9, 1),\n",
    "               'subsample': hp.quniform('subsample', 0.6, 0.95, 0.05),\n",
    "               'colsample_bytree': hp.quniform('colsample_bytree', 0.6, 0.95, 0.05),\n",
    "               'gamma': hp.loguniform('gamma', np.log(1e-8), np.log(1.0)),\n",
    "               'reg_alpha': hp.loguniform('reg_alpha', np.log(1e-8), np.log(1.0)),\n",
    "               'reg_lambda': hp.loguniform('reg_lambda', np.log(1e-6), np.log(10.0))}\n",
    "\n",
    "# define score function\n",
    "def score(params):\n",
    "    xgb = XGBRegressor(random_state=81,\n",
    "                       min_child_weight=params['min_child_weight'],\n",
    "                       max_depth=int(params['max_depth']), \n",
    "                       subsample=params['subsample'], \n",
    "                       colsample_bytree=params['colsample_bytree'],\n",
    "                       gamma=params['gamma'], \n",
    "                       reg_alpha=params['reg_alpha'], \n",
    "                       reg_lambda=params['reg_lambda'])\n",
    "    scores = cross_validate(xgb, \n",
    "                            X=X_stan, \n",
    "                            y=y, \n",
    "                            cv=5, \n",
    "                            scoring='neg_mean_squared_error', \n",
    "                            n_jobs=-1)\n",
    "    return -1 * scores['test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████| 100/100 [17:02<00:00, 10.22s/trial, best loss: 0.06332257611695011]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "             colsample_bynode=1, colsample_bytree=0.9500000000000001,\n",
       "             gamma=0.5538711006232402, gpu_id=-1, importance_type='gain',\n",
       "             interaction_constraints='', learning_rate=0.300000012,\n",
       "             max_delta_step=0, max_depth=3, min_child_weight=7.980045836795115,\n",
       "             missing=nan, monotone_constraints='()', n_estimators=100, n_jobs=0,\n",
       "             num_parallel_tree=1, random_state=81,\n",
       "             reg_alpha=6.34967732708174e-05, reg_lambda=5.6700134791633285,\n",
       "             scale_pos_weight=1, subsample=0.8500000000000001,\n",
       "             tree_method='exact', validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# run gridsearch and find best parameters\n",
    "max_evals = 100\n",
    "trials = Trials()\n",
    "history = []\n",
    "rstate = np.random.RandomState(81)\n",
    "best_params = fmin(score, param_space, algo=tpe.suggest, trials=trials, max_evals=max_evals, rstate=rstate)\n",
    "\n",
    "# refit with the best parameters\n",
    "xgb_best = XGBRegressor(random_state=81,\n",
    "                        min_child_weight=best_params['min_child_weight'],\n",
    "                        max_depth=int(best_params['max_depth']), \n",
    "                        subsample=best_params['subsample'], \n",
    "                        colsample_bytree=best_params['colsample_bytree'],\n",
    "                        gamma=best_params['gamma'], \n",
    "                        reg_alpha=best_params['reg_alpha'], \n",
    "                        reg_lambda=best_params['reg_lambda'])\n",
    "xgb_best.fit(X_stan, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute feature importance\n",
    "xgb_fi = []\n",
    "for club in clubs:\n",
    "    \n",
    "    # dummy for the seleted club = 1 (similarly, impute the max value for the club)\n",
    "    X_fi_one = X_fi_zero.copy()\n",
    "    X_fi_one[club] = X_clubs_max[club]\n",
    "    \n",
    "    # predict for each dataframe, take mean and take difference\n",
    "    pred_one = xgb_best.predict(X_fi_one.to_numpy()).mean()\n",
    "    pred_zero = xgb_best.predict(X_fi_zero.to_numpy()).mean()\n",
    "    pred_diff = pred_one - pred_zero\n",
    "    \n",
    "    # append\n",
    "    xgb_fi.append(pred_diff)\n",
    "\n",
    "# store coefficients into dataframe\n",
    "df_injury_xgb_fi = pd.DataFrame({'variable':clubs,\n",
    "                                 'injury_xgb':xgb_fi})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge all feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>ovr_la</th>\n",
       "      <th>ovr_rf</th>\n",
       "      <th>ovr_xgb</th>\n",
       "      <th>value_la</th>\n",
       "      <th>value_rf</th>\n",
       "      <th>value_xgb</th>\n",
       "      <th>skill_la</th>\n",
       "      <th>skill_rf</th>\n",
       "      <th>skill_xgb</th>\n",
       "      <th>util_la</th>\n",
       "      <th>util_rf</th>\n",
       "      <th>util_xgb</th>\n",
       "      <th>injury_la</th>\n",
       "      <th>injury_rf</th>\n",
       "      <th>injury_xgb</th>\n",
       "      <th>ovr_la_rescale</th>\n",
       "      <th>ovr_rf_rescale</th>\n",
       "      <th>ovr_xgb_rescale</th>\n",
       "      <th>value_la_rescale</th>\n",
       "      <th>value_rf_rescale</th>\n",
       "      <th>value_xgb_rescale</th>\n",
       "      <th>skill_la_rescale</th>\n",
       "      <th>skill_rf_rescale</th>\n",
       "      <th>skill_xgb_rescale</th>\n",
       "      <th>util_la_rescale</th>\n",
       "      <th>util_rf_rescale</th>\n",
       "      <th>util_xgb_rescale</th>\n",
       "      <th>injury_la_rescale</th>\n",
       "      <th>injury_rf_rescale</th>\n",
       "      <th>injury_xgb_rescale</th>\n",
       "      <th>ovr_mean</th>\n",
       "      <th>value_mean</th>\n",
       "      <th>skill_mean</th>\n",
       "      <th>util_mean</th>\n",
       "      <th>injury_mean</th>\n",
       "      <th>all_mean</th>\n",
       "      <th>league_name</th>\n",
       "      <th>club</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>club_SD Eibar</td>\n",
       "      <td>0.019042</td>\n",
       "      <td>0.015199</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006595</td>\n",
       "      <td>0.009660</td>\n",
       "      <td>0.099771</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010391</td>\n",
       "      <td>0.075505</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.004024</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000079</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61.790250</td>\n",
       "      <td>24.009951</td>\n",
       "      <td>42.569124</td>\n",
       "      <td>63.400656</td>\n",
       "      <td>58.932929</td>\n",
       "      <td>77.003780</td>\n",
       "      <td>0.0</td>\n",
       "      <td>68.654821</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>66.386140</td>\n",
       "      <td>69.512394</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.103208</td>\n",
       "      <td>32.654496</td>\n",
       "      <td>42.789775</td>\n",
       "      <td>66.445789</td>\n",
       "      <td>56.218274</td>\n",
       "      <td>78.632845</td>\n",
       "      <td>28.919234</td>\n",
       "      <td>54.601183</td>\n",
       "      <td>Spain Primera Division</td>\n",
       "      <td>SD Eibar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>club_RCD Espanyol</td>\n",
       "      <td>0.063244</td>\n",
       "      <td>0.157218</td>\n",
       "      <td>0.408159</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006468</td>\n",
       "      <td>0.033632</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002411</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.011317</td>\n",
       "      <td>0.006795</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.001998</td>\n",
       "      <td>0.0</td>\n",
       "      <td>79.706246</td>\n",
       "      <td>54.983483</td>\n",
       "      <td>60.009115</td>\n",
       "      <td>47.745949</td>\n",
       "      <td>56.646203</td>\n",
       "      <td>63.306112</td>\n",
       "      <td>0.0</td>\n",
       "      <td>49.805326</td>\n",
       "      <td>44.181770</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>73.844768</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.343963</td>\n",
       "      <td>32.654496</td>\n",
       "      <td>64.899614</td>\n",
       "      <td>55.899421</td>\n",
       "      <td>31.329032</td>\n",
       "      <td>91.281589</td>\n",
       "      <td>23.666153</td>\n",
       "      <td>53.415162</td>\n",
       "      <td>Spain Primera Division</td>\n",
       "      <td>RCD Espanyol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>club_Real Madrid</td>\n",
       "      <td>0.055333</td>\n",
       "      <td>0.048552</td>\n",
       "      <td>0.148169</td>\n",
       "      <td>0.008895</td>\n",
       "      <td>0.018871</td>\n",
       "      <td>0.068143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002582</td>\n",
       "      <td>0.014903</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.007521</td>\n",
       "      <td>-0.020011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.000425</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76.499878</td>\n",
       "      <td>31.284066</td>\n",
       "      <td>48.900144</td>\n",
       "      <td>68.860774</td>\n",
       "      <td>65.529877</td>\n",
       "      <td>70.453387</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.209603</td>\n",
       "      <td>55.199259</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>58.722243</td>\n",
       "      <td>56.753511</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.259773</td>\n",
       "      <td>32.654496</td>\n",
       "      <td>52.228030</td>\n",
       "      <td>68.281346</td>\n",
       "      <td>35.136287</td>\n",
       "      <td>71.825251</td>\n",
       "      <td>27.971423</td>\n",
       "      <td>51.088467</td>\n",
       "      <td>Spain Primera Division</td>\n",
       "      <td>Real Madrid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>club_Real Betis</td>\n",
       "      <td>0.024710</td>\n",
       "      <td>0.055922</td>\n",
       "      <td>0.172555</td>\n",
       "      <td>0.002140</td>\n",
       "      <td>0.012304</td>\n",
       "      <td>0.070932</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.003188</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000450</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.001814</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64.087678</td>\n",
       "      <td>32.891463</td>\n",
       "      <td>49.942131</td>\n",
       "      <td>52.826595</td>\n",
       "      <td>60.826080</td>\n",
       "      <td>71.031074</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.641988</td>\n",
       "      <td>44.181770</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>76.188047</td>\n",
       "      <td>69.512394</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.853294</td>\n",
       "      <td>32.654496</td>\n",
       "      <td>48.973757</td>\n",
       "      <td>61.561250</td>\n",
       "      <td>31.941253</td>\n",
       "      <td>81.900147</td>\n",
       "      <td>24.169263</td>\n",
       "      <td>49.709134</td>\n",
       "      <td>Spain Primera Division</td>\n",
       "      <td>Real Betis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>club_Sevilla FC</td>\n",
       "      <td>0.025476</td>\n",
       "      <td>0.055922</td>\n",
       "      <td>0.205011</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023001</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000835</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.000158</td>\n",
       "      <td>0.0</td>\n",
       "      <td>64.397854</td>\n",
       "      <td>32.891434</td>\n",
       "      <td>51.328906</td>\n",
       "      <td>47.745949</td>\n",
       "      <td>68.488231</td>\n",
       "      <td>56.340710</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.116468</td>\n",
       "      <td>44.181770</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>73.372246</td>\n",
       "      <td>69.512394</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.458054</td>\n",
       "      <td>32.654496</td>\n",
       "      <td>49.539398</td>\n",
       "      <td>57.524963</td>\n",
       "      <td>29.432746</td>\n",
       "      <td>80.961547</td>\n",
       "      <td>28.704183</td>\n",
       "      <td>49.232567</td>\n",
       "      <td>Spain Primera Division</td>\n",
       "      <td>Sevilla FC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>club_Brighton &amp; Hove Albion</td>\n",
       "      <td>-0.073050</td>\n",
       "      <td>-0.007629</td>\n",
       "      <td>-0.408723</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.001372</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002715</td>\n",
       "      <td>0.017714</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.023072</td>\n",
       "      <td>-0.032177</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001458</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.463182</td>\n",
       "      <td>19.031131</td>\n",
       "      <td>25.105056</td>\n",
       "      <td>47.745949</td>\n",
       "      <td>51.030969</td>\n",
       "      <td>56.340710</td>\n",
       "      <td>0.0</td>\n",
       "      <td>50.523529</td>\n",
       "      <td>57.277342</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>24.647025</td>\n",
       "      <td>48.996405</td>\n",
       "      <td>0.0</td>\n",
       "      <td>66.725037</td>\n",
       "      <td>32.654496</td>\n",
       "      <td>22.866456</td>\n",
       "      <td>51.705876</td>\n",
       "      <td>35.933624</td>\n",
       "      <td>57.881143</td>\n",
       "      <td>33.126511</td>\n",
       "      <td>40.302722</td>\n",
       "      <td>English Premier League</td>\n",
       "      <td>Brighton &amp; Hove Albion</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>club_Newcastle United</td>\n",
       "      <td>-0.073136</td>\n",
       "      <td>-0.009587</td>\n",
       "      <td>-0.288056</td>\n",
       "      <td>-0.007101</td>\n",
       "      <td>-0.003831</td>\n",
       "      <td>-0.071109</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.001846</td>\n",
       "      <td>-0.021910</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000094</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.000391</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.428359</td>\n",
       "      <td>18.604284</td>\n",
       "      <td>30.260942</td>\n",
       "      <td>30.890788</td>\n",
       "      <td>49.269546</td>\n",
       "      <td>41.613645</td>\n",
       "      <td>0.0</td>\n",
       "      <td>39.751940</td>\n",
       "      <td>27.984315</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>75.408059</td>\n",
       "      <td>69.512394</td>\n",
       "      <td>0.0</td>\n",
       "      <td>51.539508</td>\n",
       "      <td>32.654496</td>\n",
       "      <td>24.431195</td>\n",
       "      <td>40.591326</td>\n",
       "      <td>22.578751</td>\n",
       "      <td>81.640151</td>\n",
       "      <td>28.064668</td>\n",
       "      <td>39.461218</td>\n",
       "      <td>English Premier League</td>\n",
       "      <td>Newcastle United</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>club_Aston Villa</td>\n",
       "      <td>-0.097596</td>\n",
       "      <td>-0.017283</td>\n",
       "      <td>-0.413559</td>\n",
       "      <td>-0.016957</td>\n",
       "      <td>-0.017323</td>\n",
       "      <td>-0.152985</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.001375</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000285</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.000220</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.514385</td>\n",
       "      <td>16.925695</td>\n",
       "      <td>24.898390</td>\n",
       "      <td>7.495359</td>\n",
       "      <td>39.605456</td>\n",
       "      <td>24.656871</td>\n",
       "      <td>0.0</td>\n",
       "      <td>47.359423</td>\n",
       "      <td>44.181770</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>74.578092</td>\n",
       "      <td>69.512394</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.562718</td>\n",
       "      <td>32.654496</td>\n",
       "      <td>18.779490</td>\n",
       "      <td>23.919229</td>\n",
       "      <td>30.513731</td>\n",
       "      <td>81.363495</td>\n",
       "      <td>29.739071</td>\n",
       "      <td>36.863003</td>\n",
       "      <td>English Premier League</td>\n",
       "      <td>Aston Villa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>club_Burnley</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.042981</td>\n",
       "      <td>0.209826</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.023171</td>\n",
       "      <td>0.027669</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.000350</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.005678</td>\n",
       "      <td>-0.030888</td>\n",
       "      <td>-0.109024</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000168</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.072030</td>\n",
       "      <td>30.068994</td>\n",
       "      <td>51.534667</td>\n",
       "      <td>47.745949</td>\n",
       "      <td>68.610370</td>\n",
       "      <td>62.071055</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44.938774</td>\n",
       "      <td>44.181770</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.520235</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.127932</td>\n",
       "      <td>32.654496</td>\n",
       "      <td>45.225231</td>\n",
       "      <td>59.475791</td>\n",
       "      <td>29.706848</td>\n",
       "      <td>2.506745</td>\n",
       "      <td>29.594143</td>\n",
       "      <td>33.301752</td>\n",
       "      <td>English Premier League</td>\n",
       "      <td>Burnley</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>club_Sheffield United</td>\n",
       "      <td>-0.013108</td>\n",
       "      <td>0.021592</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.005797</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.005179</td>\n",
       "      <td>-0.019470</td>\n",
       "      <td>-0.000957</td>\n",
       "      <td>-0.034320</td>\n",
       "      <td>-0.099282</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.002624</td>\n",
       "      <td>0.0</td>\n",
       "      <td>48.759270</td>\n",
       "      <td>25.404122</td>\n",
       "      <td>42.569124</td>\n",
       "      <td>47.745949</td>\n",
       "      <td>56.165646</td>\n",
       "      <td>56.340710</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.879724</td>\n",
       "      <td>29.788615</td>\n",
       "      <td>83.150436</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.210928</td>\n",
       "      <td>0.0</td>\n",
       "      <td>33.200475</td>\n",
       "      <td>32.654496</td>\n",
       "      <td>38.910839</td>\n",
       "      <td>53.417435</td>\n",
       "      <td>20.556113</td>\n",
       "      <td>29.787122</td>\n",
       "      <td>21.951657</td>\n",
       "      <td>32.924633</td>\n",
       "      <td>English Premier League</td>\n",
       "      <td>Sheffield United</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>98 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       variable    ovr_la    ovr_rf   ovr_xgb  value_la  value_rf  value_xgb  skill_la  skill_rf  skill_xgb   util_la   util_rf  util_xgb  injury_la  injury_rf  injury_xgb  ovr_la_rescale  ovr_rf_rescale  ovr_xgb_rescale  value_la_rescale  value_rf_rescale  value_xgb_rescale  skill_la_rescale  skill_rf_rescale  skill_xgb_rescale  util_la_rescale  util_rf_rescale  util_xgb_rescale  injury_la_rescale  injury_rf_rescale  injury_xgb_rescale   ovr_mean  value_mean  skill_mean  util_mean  injury_mean   all_mean             league_name                    club\n",
       "76                club_SD Eibar  0.019042  0.015199  0.000000  0.006595  0.009660   0.099771       0.0  0.010391   0.075505 -0.000000 -0.004024  0.000000        0.0  -0.000079         0.0       61.790250       24.009951        42.569124         63.400656         58.932929          77.003780               0.0         68.654821         100.000000       100.000000        66.386140         69.512394                0.0          54.103208           32.654496  42.789775   66.445789   56.218274  78.632845    28.919234  54.601183  Spain Primera Division                SD Eibar\n",
       "67            club_RCD Espanyol  0.063244  0.157218  0.408159  0.000000  0.006468   0.033632       0.0  0.002411   0.000000  0.000000  0.011317  0.006795       -0.0  -0.001998         0.0       79.706246       54.983483        60.009115         47.745949         56.646203          63.306112               0.0         49.805326          44.181770       100.000000       100.000000         73.844768                0.0          38.343963           32.654496  64.899614   55.899421   31.329032  91.281589    23.666153  53.415162  Spain Primera Division            RCD Espanyol\n",
       "70             club_Real Madrid  0.055333  0.048552  0.148169  0.008895  0.018871   0.068143       0.0  0.002582   0.014903 -0.000000 -0.007521 -0.020011        0.0  -0.000425         0.0       76.499878       31.284066        48.900144         68.860774         65.529877          70.453387               0.0         50.209603          55.199259       100.000000        58.722243         56.753511                0.0          51.259773           32.654496  52.228030   68.281346   35.136287  71.825251    27.971423  51.088467  Spain Primera Division             Real Madrid\n",
       "69              club_Real Betis  0.024710  0.055922  0.172555  0.002140  0.012304   0.070932       0.0  0.003188   0.000000  0.000000  0.000450  0.000000       -0.0  -0.001814         0.0       64.087678       32.891463        49.942131         52.826595         60.826080          71.031074               0.0         51.641988          44.181770       100.000000        76.188047         69.512394                0.0          39.853294           32.654496  48.973757   61.561250   31.941253  81.900147    24.169263  49.709134  Spain Primera Division              Real Betis\n",
       "81              club_Sevilla FC  0.025476  0.055922  0.205011  0.000000  0.023001   0.000000      -0.0  0.000002   0.000000  0.000000 -0.000835  0.000000       -0.0  -0.000158         0.0       64.397854       32.891434        51.328906         47.745949         68.488231          56.340710               0.0         44.116468          44.181770       100.000000        73.372246         69.512394                0.0          53.458054           32.654496  49.539398   57.524963   29.432746  80.961547    28.704183  49.232567  Spain Primera Division              Sevilla FC\n",
       "..                          ...       ...       ...       ...       ...       ...        ...       ...       ...        ...       ...       ...       ...        ...        ...         ...             ...             ...              ...               ...               ...                ...               ...               ...                ...              ...              ...               ...                ...                ...                 ...        ...         ...         ...        ...          ...        ...                     ...                     ...\n",
       "18  club_Brighton & Hove Albion -0.073050 -0.007629 -0.408723 -0.000000 -0.001372   0.000000       0.0  0.002715   0.017714 -0.000000 -0.023072 -0.032177        0.0   0.001458         0.0       24.463182       19.031131        25.105056         47.745949         51.030969          56.340710               0.0         50.523529          57.277342       100.000000        24.647025         48.996405                0.0          66.725037           32.654496  22.866456   51.705876   35.933624  57.881143    33.126511  40.302722  English Premier League  Brighton & Hove Albion\n",
       "56        club_Newcastle United -0.073136 -0.009587 -0.288056 -0.007101 -0.003831  -0.071109      -0.0 -0.001846  -0.021910  0.000000  0.000094  0.000000       -0.0  -0.000391         0.0       24.428359       18.604284        30.260942         30.890788         49.269546          41.613645               0.0         39.751940          27.984315       100.000000        75.408059         69.512394                0.0          51.539508           32.654496  24.431195   40.591326   22.578751  81.640151    28.064668  39.461218  English Premier League        Newcastle United\n",
       "8              club_Aston Villa -0.097596 -0.017283 -0.413559 -0.016957 -0.017323  -0.152985      -0.0  0.001375   0.000000  0.000000 -0.000285  0.000000       -0.0   0.000220         0.0       14.514385       16.925695        24.898390          7.495359         39.605456          24.656871               0.0         47.359423          44.181770       100.000000        74.578092         69.512394                0.0          56.562718           32.654496  18.779490   23.919229   30.513731  81.363495    29.739071  36.863003  English Premier League             Aston Villa\n",
       "19                 club_Burnley  0.000000  0.042981  0.209826  0.000000  0.023171   0.027669      -0.0  0.000350   0.000000 -0.005678 -0.030888 -0.109024        0.0   0.000168         0.0       54.072030       30.068994        51.534667         47.745949         68.610370          62.071055               0.0         44.938774          44.181770         0.000000         7.520235          0.000000                0.0          56.127932           32.654496  45.225231   59.475791   29.706848   2.506745    29.594143  33.301752  English Premier League                 Burnley\n",
       "82        club_Sheffield United -0.013108  0.021592  0.000000 -0.000000  0.005797   0.000000      -0.0 -0.005179  -0.019470 -0.000957 -0.034320 -0.099282       -0.0  -0.002624         0.0       48.759270       25.404122        42.569124         47.745949         56.165646          56.340710               0.0         31.879724          29.788615        83.150436         0.000000          6.210928                0.0          33.200475           32.654496  38.910839   53.417435   20.556113  29.787122    21.951657  32.924633  English Premier League        Sheffield United\n",
       "\n",
       "[98 rows x 39 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge all dataframes\n",
    "merge_dfs = [df_ovr_la_fi, df_ovr_rf_fi, df_ovr_xgb_fi,\n",
    "             df_value_la_fi, df_value_rf_fi, df_value_xgb_fi,\n",
    "             df_skill_la_fi, df_skill_rf_fi, df_skill_xgb_fi,\n",
    "             df_util_la_fi, df_util_rf_fi, df_util_xgb_fi,\n",
    "             df_injury_la_fi, df_injury_rf_fi, df_injury_xgb_fi]\n",
    "df_c_fi = functools.reduce(lambda  left,right: pd.merge(left,right,on=['variable']), merge_dfs)\n",
    "\n",
    "# rescale all feature importance\n",
    "targets = ['ovr', 'value', 'skill', 'util', 'injury']\n",
    "models = ['la', 'rf', 'xgb']\n",
    "all_pattern = []\n",
    "for target in targets:\n",
    "    for model in models:\n",
    "        all_pattern.append(target + '_' + model)\n",
    "\n",
    "rescale_scores = []\n",
    "for pattern in all_pattern:\n",
    "    df_c_fi[pattern + '_rescale'] = MinMaxScaler().fit_transform(df_c_fi[[pattern]]) * 100\n",
    "    rescale_scores.append(pattern + '_rescale')\n",
    "\n",
    "# mean of each section\n",
    "for target in targets:\n",
    "    section_scores = [x for x in rescale_scores if x.startswith(target)]\n",
    "    df_c_fi[target + '_mean'] = df_c_fi[section_scores].mean(axis=1)\n",
    "\n",
    "# calculate mean of all scores \n",
    "df_c_fi['all_mean'] = df_c_fi[rescale_scores].mean(axis=1)\n",
    "\n",
    "# merge league name\n",
    "df_c_fi = df_c_fi.merge(df_league_club, on='variable')\n",
    "df_c_fi = df_c_fi.sort_values(['league_name','all_mean'], ascending=False)\n",
    "df_c_fi.to_csv('data/df_c_fi.csv', index=False)\n",
    "df_c_fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do  \n",
    "- Overall score distribution for each club (kernel density by year, 4 row * 5 column, 5 different figures)\n",
    "- Change in overall score distribution for each club (same)\n",
    "- Visualization of yearly change in club score\n",
    "- Visualization of top 5 clubs in club score on map?\n",
    "- radar chart by league\n",
    "- radar chart for top 6 teams\n",
    "- distribution for each indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
